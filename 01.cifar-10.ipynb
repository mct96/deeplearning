{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4849e80b-9d70-4702-bbee-726acbd1a8c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import *\n",
    "from torchvision.transforms import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a0289f-3534-436c-b7c2-f826b99f80a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53defcd-7967-42d4-8189-99351544061d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "t1 = Compose([\n",
    "    RandomHorizontalFlip(p=.5),\n",
    "    RandomRotation((-0.25, +0.25)),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "t2 = Compose([\n",
    "    ColorJitter((0.90, 1), (0.90, 1)),\n",
    "    RandomGrayscale(p=0.3),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "t3 = Compose([\n",
    "    RandomResizedCrop((32, 32), ratio=(0.70, 1.3)),\n",
    "    RandomRotation((-0.25, +0.25)),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "tt = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "t1 = torchvision.datasets.CIFAR10(root=\"./\", train=True, transform=t1, download=True)\n",
    "t2 = torchvision.datasets.CIFAR10(root=\"./\", train=True, transform=t2, download=True)\n",
    "t3 = torchvision.datasets.CIFAR10(root=\"./\", train=True, transform=t3, download=True)\n",
    "train_data = ConcatDataset([t1, t2, t3])\n",
    "test_data = torchvision.datasets.CIFAR10(root=\"./\", train=False, transform=tt, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a1b1ae-a9ed-49a1-b835-a8bd5f8db624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3, 4)\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        axs[i][j].imshow(train_data[random.choice(range(len(train_data)))][0].numpy().transpose((1, 2, 0)))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52abb043-5152-4e1f-8452-956fd621c0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 18, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(18, 32, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        logits = self.model(X)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85bdc3d-0ad7-4d64-bae8-031e0e0815e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812b3499-c78e-45b8-b393-652f45bdfd99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = Network().to(device)\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f71cd30-1401-4f7b-b2f4-a297f7263bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 0\n",
      "loss: 2.311264  [   64/150000]\n",
      "loss: 1.917317  [ 6464/150000]\n",
      "loss: 1.654202  [12864/150000]\n",
      "loss: 1.792289  [19264/150000]\n",
      "loss: 1.557274  [25664/150000]\n",
      "loss: 1.544401  [32064/150000]\n",
      "loss: 1.513479  [38464/150000]\n",
      "loss: 1.373415  [44864/150000]\n",
      "loss: 1.346147  [51264/150000]\n",
      "loss: 1.389546  [57664/150000]\n",
      "loss: 1.357974  [64064/150000]\n",
      "loss: 1.300649  [70464/150000]\n",
      "loss: 1.329045  [76864/150000]\n",
      "loss: 1.269755  [83264/150000]\n",
      "loss: 1.350940  [89664/150000]\n",
      "loss: 1.112824  [96064/150000]\n",
      "loss: 1.984950  [102464/150000]\n",
      "loss: 1.502107  [108864/150000]\n",
      "loss: 1.664114  [115264/150000]\n",
      "loss: 1.450989  [121664/150000]\n",
      "loss: 1.210979  [128064/150000]\n",
      "loss: 1.684574  [134464/150000]\n",
      "loss: 1.367869  [140864/150000]\n",
      "loss: 1.354412  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 1.193256 \n",
      "\n",
      "training epoch 1\n",
      "loss: 1.424752  [   64/150000]\n",
      "loss: 1.011075  [ 6464/150000]\n",
      "loss: 0.983315  [12864/150000]\n",
      "loss: 1.028205  [19264/150000]\n",
      "loss: 0.995602  [25664/150000]\n",
      "loss: 1.130865  [32064/150000]\n",
      "loss: 0.912217  [38464/150000]\n",
      "loss: 0.926500  [44864/150000]\n",
      "loss: 0.711811  [51264/150000]\n",
      "loss: 1.030540  [57664/150000]\n",
      "loss: 0.842377  [64064/150000]\n",
      "loss: 0.983599  [70464/150000]\n",
      "loss: 0.911282  [76864/150000]\n",
      "loss: 0.605810  [83264/150000]\n",
      "loss: 0.895540  [89664/150000]\n",
      "loss: 0.704046  [96064/150000]\n",
      "loss: 1.609845  [102464/150000]\n",
      "loss: 1.274654  [108864/150000]\n",
      "loss: 1.281790  [115264/150000]\n",
      "loss: 1.267390  [121664/150000]\n",
      "loss: 1.161793  [128064/150000]\n",
      "loss: 1.374511  [134464/150000]\n",
      "loss: 1.260032  [140864/150000]\n",
      "loss: 1.223426  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.973056 \n",
      "\n",
      "training epoch 2\n",
      "loss: 1.227148  [   64/150000]\n",
      "loss: 0.747391  [ 6464/150000]\n",
      "loss: 0.683026  [12864/150000]\n",
      "loss: 0.858050  [19264/150000]\n",
      "loss: 0.689441  [25664/150000]\n",
      "loss: 0.937677  [32064/150000]\n",
      "loss: 0.706542  [38464/150000]\n",
      "loss: 0.869638  [44864/150000]\n",
      "loss: 0.423177  [51264/150000]\n",
      "loss: 0.785069  [57664/150000]\n",
      "loss: 0.612946  [64064/150000]\n",
      "loss: 0.827293  [70464/150000]\n",
      "loss: 0.725012  [76864/150000]\n",
      "loss: 0.548870  [83264/150000]\n",
      "loss: 0.731511  [89664/150000]\n",
      "loss: 0.554818  [96064/150000]\n",
      "loss: 1.195026  [102464/150000]\n",
      "loss: 1.260420  [108864/150000]\n",
      "loss: 1.301047  [115264/150000]\n",
      "loss: 1.123374  [121664/150000]\n",
      "loss: 1.133322  [128064/150000]\n",
      "loss: 1.081899  [134464/150000]\n",
      "loss: 1.427576  [140864/150000]\n",
      "loss: 1.344565  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.842532 \n",
      "\n",
      "training epoch 3\n",
      "loss: 0.900796  [   64/150000]\n",
      "loss: 0.735340  [ 6464/150000]\n",
      "loss: 0.589967  [12864/150000]\n",
      "loss: 0.717233  [19264/150000]\n",
      "loss: 0.584087  [25664/150000]\n",
      "loss: 0.913697  [32064/150000]\n",
      "loss: 0.633170  [38464/150000]\n",
      "loss: 0.764680  [44864/150000]\n",
      "loss: 0.393745  [51264/150000]\n",
      "loss: 0.520106  [57664/150000]\n",
      "loss: 0.525763  [64064/150000]\n",
      "loss: 0.812546  [70464/150000]\n",
      "loss: 0.513367  [76864/150000]\n",
      "loss: 0.415900  [83264/150000]\n",
      "loss: 0.627940  [89664/150000]\n",
      "loss: 0.570427  [96064/150000]\n",
      "loss: 1.186984  [102464/150000]\n",
      "loss: 1.160500  [108864/150000]\n",
      "loss: 1.128935  [115264/150000]\n",
      "loss: 0.962828  [121664/150000]\n",
      "loss: 1.436460  [128064/150000]\n",
      "loss: 1.058957  [134464/150000]\n",
      "loss: 1.212871  [140864/150000]\n",
      "loss: 0.963539  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.854524 \n",
      "\n",
      "training epoch 4\n",
      "loss: 0.793175  [   64/150000]\n",
      "loss: 0.600788  [ 6464/150000]\n",
      "loss: 0.552688  [12864/150000]\n",
      "loss: 0.633404  [19264/150000]\n",
      "loss: 0.581455  [25664/150000]\n",
      "loss: 0.640981  [32064/150000]\n",
      "loss: 0.511568  [38464/150000]\n",
      "loss: 0.694347  [44864/150000]\n",
      "loss: 0.324622  [51264/150000]\n",
      "loss: 0.572540  [57664/150000]\n",
      "loss: 0.427934  [64064/150000]\n",
      "loss: 0.599524  [70464/150000]\n",
      "loss: 0.387778  [76864/150000]\n",
      "loss: 0.456489  [83264/150000]\n",
      "loss: 0.537579  [89664/150000]\n",
      "loss: 0.446710  [96064/150000]\n",
      "loss: 1.132617  [102464/150000]\n",
      "loss: 1.085539  [108864/150000]\n",
      "loss: 1.241188  [115264/150000]\n",
      "loss: 1.172009  [121664/150000]\n",
      "loss: 1.147828  [128064/150000]\n",
      "loss: 1.038023  [134464/150000]\n",
      "loss: 1.106622  [140864/150000]\n",
      "loss: 0.993453  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.853187 \n",
      "\n",
      "training epoch 5\n",
      "loss: 0.761742  [   64/150000]\n",
      "loss: 0.556119  [ 6464/150000]\n",
      "loss: 0.423848  [12864/150000]\n",
      "loss: 0.662760  [19264/150000]\n",
      "loss: 0.603060  [25664/150000]\n",
      "loss: 0.793246  [32064/150000]\n",
      "loss: 0.497140  [38464/150000]\n",
      "loss: 0.809816  [44864/150000]\n",
      "loss: 0.437697  [51264/150000]\n",
      "loss: 0.513713  [57664/150000]\n",
      "loss: 0.477626  [64064/150000]\n",
      "loss: 0.567833  [70464/150000]\n",
      "loss: 0.509219  [76864/150000]\n",
      "loss: 0.481832  [83264/150000]\n",
      "loss: 0.524359  [89664/150000]\n",
      "loss: 0.478427  [96064/150000]\n",
      "loss: 1.145473  [102464/150000]\n",
      "loss: 1.195296  [108864/150000]\n",
      "loss: 1.068657  [115264/150000]\n",
      "loss: 0.908859  [121664/150000]\n",
      "loss: 1.153540  [128064/150000]\n",
      "loss: 1.228429  [134464/150000]\n",
      "loss: 1.106807  [140864/150000]\n",
      "loss: 1.162162  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.803015 \n",
      "\n",
      "training epoch 6\n",
      "loss: 0.631836  [   64/150000]\n",
      "loss: 0.521370  [ 6464/150000]\n",
      "loss: 0.408055  [12864/150000]\n",
      "loss: 0.684433  [19264/150000]\n",
      "loss: 0.568157  [25664/150000]\n",
      "loss: 0.552719  [32064/150000]\n",
      "loss: 0.389359  [38464/150000]\n",
      "loss: 0.599918  [44864/150000]\n",
      "loss: 0.362553  [51264/150000]\n",
      "loss: 0.453796  [57664/150000]\n",
      "loss: 0.301786  [64064/150000]\n",
      "loss: 0.545117  [70464/150000]\n",
      "loss: 0.486798  [76864/150000]\n",
      "loss: 0.405117  [83264/150000]\n",
      "loss: 0.495454  [89664/150000]\n",
      "loss: 0.459187  [96064/150000]\n",
      "loss: 1.035452  [102464/150000]\n",
      "loss: 1.175941  [108864/150000]\n",
      "loss: 1.272770  [115264/150000]\n",
      "loss: 0.952538  [121664/150000]\n",
      "loss: 0.892616  [128064/150000]\n",
      "loss: 1.059057  [134464/150000]\n",
      "loss: 0.975377  [140864/150000]\n",
      "loss: 0.977952  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.808394 \n",
      "\n",
      "training epoch 7\n",
      "loss: 0.755424  [   64/150000]\n",
      "loss: 0.480597  [ 6464/150000]\n",
      "loss: 0.372422  [12864/150000]\n",
      "loss: 0.593984  [19264/150000]\n",
      "loss: 0.458025  [25664/150000]\n",
      "loss: 0.609054  [32064/150000]\n",
      "loss: 0.389323  [38464/150000]\n",
      "loss: 0.637655  [44864/150000]\n",
      "loss: 0.285015  [51264/150000]\n",
      "loss: 0.272002  [57664/150000]\n",
      "loss: 0.337413  [64064/150000]\n",
      "loss: 0.475246  [70464/150000]\n",
      "loss: 0.279312  [76864/150000]\n",
      "loss: 0.381504  [83264/150000]\n",
      "loss: 0.379867  [89664/150000]\n",
      "loss: 0.355938  [96064/150000]\n",
      "loss: 1.311340  [102464/150000]\n",
      "loss: 1.071085  [108864/150000]\n",
      "loss: 0.913988  [115264/150000]\n",
      "loss: 0.941633  [121664/150000]\n",
      "loss: 1.161797  [128064/150000]\n",
      "loss: 1.010382  [134464/150000]\n",
      "loss: 1.078210  [140864/150000]\n",
      "loss: 0.994563  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.794358 \n",
      "\n",
      "training epoch 8\n",
      "loss: 0.678697  [   64/150000]\n",
      "loss: 0.377586  [ 6464/150000]\n",
      "loss: 0.283636  [12864/150000]\n",
      "loss: 0.596283  [19264/150000]\n",
      "loss: 0.555359  [25664/150000]\n",
      "loss: 0.474229  [32064/150000]\n",
      "loss: 0.456693  [38464/150000]\n",
      "loss: 0.608299  [44864/150000]\n",
      "loss: 0.287628  [51264/150000]\n",
      "loss: 0.260299  [57664/150000]\n",
      "loss: 0.375602  [64064/150000]\n",
      "loss: 0.404519  [70464/150000]\n",
      "loss: 0.413392  [76864/150000]\n",
      "loss: 0.333357  [83264/150000]\n",
      "loss: 0.318423  [89664/150000]\n",
      "loss: 0.284554  [96064/150000]\n",
      "loss: 1.198507  [102464/150000]\n",
      "loss: 1.163681  [108864/150000]\n",
      "loss: 1.127009  [115264/150000]\n",
      "loss: 1.068277  [121664/150000]\n",
      "loss: 1.062086  [128064/150000]\n",
      "loss: 0.947880  [134464/150000]\n",
      "loss: 0.966282  [140864/150000]\n",
      "loss: 1.092589  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.778565 \n",
      "\n",
      "training epoch 9\n",
      "loss: 0.489812  [   64/150000]\n",
      "loss: 0.478237  [ 6464/150000]\n",
      "loss: 0.388247  [12864/150000]\n",
      "loss: 0.668327  [19264/150000]\n",
      "loss: 0.405807  [25664/150000]\n",
      "loss: 0.590273  [32064/150000]\n",
      "loss: 0.468376  [38464/150000]\n",
      "loss: 0.490234  [44864/150000]\n",
      "loss: 0.267593  [51264/150000]\n",
      "loss: 0.347794  [57664/150000]\n",
      "loss: 0.249046  [64064/150000]\n",
      "loss: 0.473793  [70464/150000]\n",
      "loss: 0.360656  [76864/150000]\n",
      "loss: 0.286715  [83264/150000]\n",
      "loss: 0.346348  [89664/150000]\n",
      "loss: 0.292143  [96064/150000]\n",
      "loss: 1.225226  [102464/150000]\n",
      "loss: 0.956657  [108864/150000]\n",
      "loss: 1.065227  [115264/150000]\n",
      "loss: 1.246990  [121664/150000]\n",
      "loss: 1.095512  [128064/150000]\n",
      "loss: 1.141034  [134464/150000]\n",
      "loss: 1.025082  [140864/150000]\n",
      "loss: 1.008831  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.808129 \n",
      "\n",
      "training epoch 10\n",
      "loss: 0.669557  [   64/150000]\n",
      "loss: 0.361183  [ 6464/150000]\n",
      "loss: 0.269757  [12864/150000]\n",
      "loss: 0.549311  [19264/150000]\n",
      "loss: 0.367165  [25664/150000]\n",
      "loss: 0.454141  [32064/150000]\n",
      "loss: 0.294371  [38464/150000]\n",
      "loss: 0.680606  [44864/150000]\n",
      "loss: 0.257887  [51264/150000]\n",
      "loss: 0.313999  [57664/150000]\n",
      "loss: 0.142802  [64064/150000]\n",
      "loss: 0.270551  [70464/150000]\n",
      "loss: 0.159620  [76864/150000]\n",
      "loss: 0.321081  [83264/150000]\n",
      "loss: 0.353669  [89664/150000]\n",
      "loss: 0.150632  [96064/150000]\n",
      "loss: 1.221344  [102464/150000]\n",
      "loss: 0.950282  [108864/150000]\n",
      "loss: 1.102736  [115264/150000]\n",
      "loss: 0.983608  [121664/150000]\n",
      "loss: 0.911459  [128064/150000]\n",
      "loss: 1.215780  [134464/150000]\n",
      "loss: 1.055060  [140864/150000]\n",
      "loss: 0.919414  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.761690 \n",
      "\n",
      "training epoch 11\n",
      "loss: 0.464192  [   64/150000]\n",
      "loss: 0.328552  [ 6464/150000]\n",
      "loss: 0.331545  [12864/150000]\n",
      "loss: 0.491051  [19264/150000]\n",
      "loss: 0.465654  [25664/150000]\n",
      "loss: 0.483659  [32064/150000]\n",
      "loss: 0.318185  [38464/150000]\n",
      "loss: 0.590787  [44864/150000]\n",
      "loss: 0.241539  [51264/150000]\n",
      "loss: 0.311852  [57664/150000]\n",
      "loss: 0.254373  [64064/150000]\n",
      "loss: 0.296691  [70464/150000]\n",
      "loss: 0.418389  [76864/150000]\n",
      "loss: 0.297245  [83264/150000]\n",
      "loss: 0.359131  [89664/150000]\n",
      "loss: 0.226787  [96064/150000]\n",
      "loss: 0.880638  [102464/150000]\n",
      "loss: 1.065435  [108864/150000]\n",
      "loss: 1.200873  [115264/150000]\n",
      "loss: 1.015701  [121664/150000]\n",
      "loss: 0.964231  [128064/150000]\n",
      "loss: 1.061622  [134464/150000]\n",
      "loss: 1.107848  [140864/150000]\n",
      "loss: 0.921123  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.784772 \n",
      "\n",
      "training epoch 12\n",
      "loss: 0.511427  [   64/150000]\n",
      "loss: 0.397934  [ 6464/150000]\n",
      "loss: 0.305420  [12864/150000]\n",
      "loss: 0.350058  [19264/150000]\n",
      "loss: 0.310157  [25664/150000]\n",
      "loss: 0.349620  [32064/150000]\n",
      "loss: 0.334310  [38464/150000]\n",
      "loss: 0.522685  [44864/150000]\n",
      "loss: 0.256128  [51264/150000]\n",
      "loss: 0.336550  [57664/150000]\n",
      "loss: 0.171325  [64064/150000]\n",
      "loss: 0.373391  [70464/150000]\n",
      "loss: 0.236449  [76864/150000]\n",
      "loss: 0.305253  [83264/150000]\n",
      "loss: 0.320555  [89664/150000]\n",
      "loss: 0.106909  [96064/150000]\n",
      "loss: 1.065756  [102464/150000]\n",
      "loss: 0.856624  [108864/150000]\n",
      "loss: 1.086447  [115264/150000]\n",
      "loss: 0.881804  [121664/150000]\n",
      "loss: 1.058175  [128064/150000]\n",
      "loss: 1.133080  [134464/150000]\n",
      "loss: 0.907680  [140864/150000]\n",
      "loss: 0.906970  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.763631 \n",
      "\n",
      "training epoch 13\n",
      "loss: 0.546474  [   64/150000]\n",
      "loss: 0.278162  [ 6464/150000]\n",
      "loss: 0.317591  [12864/150000]\n",
      "loss: 0.472207  [19264/150000]\n",
      "loss: 0.360433  [25664/150000]\n",
      "loss: 0.412344  [32064/150000]\n",
      "loss: 0.503007  [38464/150000]\n",
      "loss: 0.441883  [44864/150000]\n",
      "loss: 0.217641  [51264/150000]\n",
      "loss: 0.339140  [57664/150000]\n",
      "loss: 0.156474  [64064/150000]\n",
      "loss: 0.334105  [70464/150000]\n",
      "loss: 0.231697  [76864/150000]\n",
      "loss: 0.181380  [83264/150000]\n",
      "loss: 0.295367  [89664/150000]\n",
      "loss: 0.248874  [96064/150000]\n",
      "loss: 0.963885  [102464/150000]\n",
      "loss: 1.100427  [108864/150000]\n",
      "loss: 1.332448  [115264/150000]\n",
      "loss: 0.953251  [121664/150000]\n",
      "loss: 1.104902  [128064/150000]\n",
      "loss: 0.911184  [134464/150000]\n",
      "loss: 1.114407  [140864/150000]\n",
      "loss: 0.835855  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.775611 \n",
      "\n",
      "training epoch 14\n",
      "loss: 0.577825  [   64/150000]\n",
      "loss: 0.340168  [ 6464/150000]\n",
      "loss: 0.206562  [12864/150000]\n",
      "loss: 0.447176  [19264/150000]\n",
      "loss: 0.289851  [25664/150000]\n",
      "loss: 0.344708  [32064/150000]\n",
      "loss: 0.211815  [38464/150000]\n",
      "loss: 0.581328  [44864/150000]\n",
      "loss: 0.194936  [51264/150000]\n",
      "loss: 0.268430  [57664/150000]\n",
      "loss: 0.201591  [64064/150000]\n",
      "loss: 0.142288  [70464/150000]\n",
      "loss: 0.140342  [76864/150000]\n",
      "loss: 0.150216  [83264/150000]\n",
      "loss: 0.335579  [89664/150000]\n",
      "loss: 0.185382  [96064/150000]\n",
      "loss: 1.137939  [102464/150000]\n",
      "loss: 1.221760  [108864/150000]\n",
      "loss: 1.003210  [115264/150000]\n",
      "loss: 0.754163  [121664/150000]\n",
      "loss: 1.002607  [128064/150000]\n",
      "loss: 0.996541  [134464/150000]\n",
      "loss: 1.177394  [140864/150000]\n",
      "loss: 0.930196  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.823528 \n",
      "\n",
      "training epoch 15\n",
      "loss: 0.392659  [   64/150000]\n",
      "loss: 0.232868  [ 6464/150000]\n",
      "loss: 0.239563  [12864/150000]\n",
      "loss: 0.254725  [19264/150000]\n",
      "loss: 0.181290  [25664/150000]\n",
      "loss: 0.363245  [32064/150000]\n",
      "loss: 0.319750  [38464/150000]\n",
      "loss: 0.535040  [44864/150000]\n",
      "loss: 0.229590  [51264/150000]\n",
      "loss: 0.293145  [57664/150000]\n",
      "loss: 0.295969  [64064/150000]\n",
      "loss: 0.188294  [70464/150000]\n",
      "loss: 0.243806  [76864/150000]\n",
      "loss: 0.150009  [83264/150000]\n",
      "loss: 0.293054  [89664/150000]\n",
      "loss: 0.156696  [96064/150000]\n",
      "loss: 1.058268  [102464/150000]\n",
      "loss: 1.103797  [108864/150000]\n",
      "loss: 1.222147  [115264/150000]\n",
      "loss: 1.032579  [121664/150000]\n",
      "loss: 1.048621  [128064/150000]\n",
      "loss: 0.978163  [134464/150000]\n",
      "loss: 1.290384  [140864/150000]\n",
      "loss: 1.035472  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.817310 \n",
      "\n",
      "training epoch 16\n",
      "loss: 0.475680  [   64/150000]\n",
      "loss: 0.231644  [ 6464/150000]\n",
      "loss: 0.254068  [12864/150000]\n",
      "loss: 0.198671  [19264/150000]\n",
      "loss: 0.228125  [25664/150000]\n",
      "loss: 0.500974  [32064/150000]\n",
      "loss: 0.382979  [38464/150000]\n",
      "loss: 0.222087  [44864/150000]\n",
      "loss: 0.161345  [51264/150000]\n",
      "loss: 0.253309  [57664/150000]\n",
      "loss: 0.234073  [64064/150000]\n",
      "loss: 0.225737  [70464/150000]\n",
      "loss: 0.269987  [76864/150000]\n",
      "loss: 0.155056  [83264/150000]\n",
      "loss: 0.338488  [89664/150000]\n",
      "loss: 0.205039  [96064/150000]\n",
      "loss: 0.928074  [102464/150000]\n",
      "loss: 0.905150  [108864/150000]\n",
      "loss: 1.137712  [115264/150000]\n",
      "loss: 1.102015  [121664/150000]\n",
      "loss: 1.193781  [128064/150000]\n",
      "loss: 1.069800  [134464/150000]\n",
      "loss: 1.059675  [140864/150000]\n",
      "loss: 0.881299  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.777762 \n",
      "\n",
      "training epoch 17\n",
      "loss: 0.453307  [   64/150000]\n",
      "loss: 0.257657  [ 6464/150000]\n",
      "loss: 0.167920  [12864/150000]\n",
      "loss: 0.347537  [19264/150000]\n",
      "loss: 0.237258  [25664/150000]\n",
      "loss: 0.294505  [32064/150000]\n",
      "loss: 0.255395  [38464/150000]\n",
      "loss: 0.367470  [44864/150000]\n",
      "loss: 0.173838  [51264/150000]\n",
      "loss: 0.152310  [57664/150000]\n",
      "loss: 0.120644  [64064/150000]\n",
      "loss: 0.197466  [70464/150000]\n",
      "loss: 0.085834  [76864/150000]\n",
      "loss: 0.153414  [83264/150000]\n",
      "loss: 0.332262  [89664/150000]\n",
      "loss: 0.121969  [96064/150000]\n",
      "loss: 1.185083  [102464/150000]\n",
      "loss: 0.884471  [108864/150000]\n",
      "loss: 0.902447  [115264/150000]\n",
      "loss: 1.040567  [121664/150000]\n",
      "loss: 1.092156  [128064/150000]\n",
      "loss: 0.975872  [134464/150000]\n",
      "loss: 1.033240  [140864/150000]\n",
      "loss: 0.936773  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.892808 \n",
      "\n",
      "training epoch 18\n",
      "loss: 0.509074  [   64/150000]\n",
      "loss: 0.245389  [ 6464/150000]\n",
      "loss: 0.196530  [12864/150000]\n",
      "loss: 0.334679  [19264/150000]\n",
      "loss: 0.293596  [25664/150000]\n",
      "loss: 0.300324  [32064/150000]\n",
      "loss: 0.485161  [38464/150000]\n",
      "loss: 0.408480  [44864/150000]\n",
      "loss: 0.136201  [51264/150000]\n",
      "loss: 0.166150  [57664/150000]\n",
      "loss: 0.176597  [64064/150000]\n",
      "loss: 0.174552  [70464/150000]\n",
      "loss: 0.174351  [76864/150000]\n",
      "loss: 0.139875  [83264/150000]\n",
      "loss: 0.263720  [89664/150000]\n",
      "loss: 0.258462  [96064/150000]\n",
      "loss: 1.036167  [102464/150000]\n",
      "loss: 0.996541  [108864/150000]\n",
      "loss: 0.926095  [115264/150000]\n",
      "loss: 1.035838  [121664/150000]\n",
      "loss: 0.760958  [128064/150000]\n",
      "loss: 0.911198  [134464/150000]\n",
      "loss: 1.186949  [140864/150000]\n",
      "loss: 0.834897  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.767963 \n",
      "\n",
      "training epoch 19\n",
      "loss: 0.266090  [   64/150000]\n",
      "loss: 0.284616  [ 6464/150000]\n",
      "loss: 0.197180  [12864/150000]\n",
      "loss: 0.349853  [19264/150000]\n",
      "loss: 0.226076  [25664/150000]\n",
      "loss: 0.271458  [32064/150000]\n",
      "loss: 0.394116  [38464/150000]\n",
      "loss: 0.534082  [44864/150000]\n",
      "loss: 0.223950  [51264/150000]\n",
      "loss: 0.162175  [57664/150000]\n",
      "loss: 0.122295  [64064/150000]\n",
      "loss: 0.107961  [70464/150000]\n",
      "loss: 0.191558  [76864/150000]\n",
      "loss: 0.156531  [83264/150000]\n",
      "loss: 0.276085  [89664/150000]\n",
      "loss: 0.179641  [96064/150000]\n",
      "loss: 1.088999  [102464/150000]\n",
      "loss: 0.933980  [108864/150000]\n",
      "loss: 0.998241  [115264/150000]\n",
      "loss: 0.988975  [121664/150000]\n",
      "loss: 1.062490  [128064/150000]\n",
      "loss: 1.062681  [134464/150000]\n",
      "loss: 1.073801  [140864/150000]\n",
      "loss: 1.032998  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.765905 \n",
      "\n",
      "training epoch 20\n",
      "loss: 0.441226  [   64/150000]\n",
      "loss: 0.347953  [ 6464/150000]\n",
      "loss: 0.243130  [12864/150000]\n",
      "loss: 0.249579  [19264/150000]\n",
      "loss: 0.169159  [25664/150000]\n",
      "loss: 0.280960  [32064/150000]\n",
      "loss: 0.264382  [38464/150000]\n",
      "loss: 0.398088  [44864/150000]\n",
      "loss: 0.192459  [51264/150000]\n",
      "loss: 0.080167  [57664/150000]\n",
      "loss: 0.206419  [64064/150000]\n",
      "loss: 0.202550  [70464/150000]\n",
      "loss: 0.177200  [76864/150000]\n",
      "loss: 0.195341  [83264/150000]\n",
      "loss: 0.241386  [89664/150000]\n",
      "loss: 0.060920  [96064/150000]\n",
      "loss: 1.031408  [102464/150000]\n",
      "loss: 0.982177  [108864/150000]\n",
      "loss: 1.113097  [115264/150000]\n",
      "loss: 0.990058  [121664/150000]\n",
      "loss: 1.026641  [128064/150000]\n",
      "loss: 0.934391  [134464/150000]\n",
      "loss: 0.975201  [140864/150000]\n",
      "loss: 0.806177  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.774376 \n",
      "\n",
      "training epoch 21\n",
      "loss: 0.570820  [   64/150000]\n",
      "loss: 0.241292  [ 6464/150000]\n",
      "loss: 0.164269  [12864/150000]\n",
      "loss: 0.294897  [19264/150000]\n",
      "loss: 0.281767  [25664/150000]\n",
      "loss: 0.359232  [32064/150000]\n",
      "loss: 0.340746  [38464/150000]\n",
      "loss: 0.336926  [44864/150000]\n",
      "loss: 0.064002  [51264/150000]\n",
      "loss: 0.099189  [57664/150000]\n",
      "loss: 0.107913  [64064/150000]\n",
      "loss: 0.318541  [70464/150000]\n",
      "loss: 0.110671  [76864/150000]\n",
      "loss: 0.187557  [83264/150000]\n",
      "loss: 0.169175  [89664/150000]\n",
      "loss: 0.269137  [96064/150000]\n",
      "loss: 0.974838  [102464/150000]\n",
      "loss: 1.200057  [108864/150000]\n",
      "loss: 1.231483  [115264/150000]\n",
      "loss: 0.912180  [121664/150000]\n",
      "loss: 0.980767  [128064/150000]\n",
      "loss: 0.995538  [134464/150000]\n",
      "loss: 0.935106  [140864/150000]\n",
      "loss: 0.873438  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.755325 \n",
      "\n",
      "training epoch 22\n",
      "loss: 0.707473  [   64/150000]\n",
      "loss: 0.232822  [ 6464/150000]\n",
      "loss: 0.154132  [12864/150000]\n",
      "loss: 0.431356  [19264/150000]\n",
      "loss: 0.174828  [25664/150000]\n",
      "loss: 0.314290  [32064/150000]\n",
      "loss: 0.277665  [38464/150000]\n",
      "loss: 0.267521  [44864/150000]\n",
      "loss: 0.101707  [51264/150000]\n",
      "loss: 0.098839  [57664/150000]\n",
      "loss: 0.091727  [64064/150000]\n",
      "loss: 0.159023  [70464/150000]\n",
      "loss: 0.156103  [76864/150000]\n",
      "loss: 0.123050  [83264/150000]\n",
      "loss: 0.140720  [89664/150000]\n",
      "loss: 0.102809  [96064/150000]\n",
      "loss: 0.947647  [102464/150000]\n",
      "loss: 0.988179  [108864/150000]\n",
      "loss: 1.098588  [115264/150000]\n",
      "loss: 0.813349  [121664/150000]\n",
      "loss: 0.999922  [128064/150000]\n",
      "loss: 1.215961  [134464/150000]\n",
      "loss: 1.079832  [140864/150000]\n",
      "loss: 1.070912  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.811455 \n",
      "\n",
      "training epoch 23\n",
      "loss: 0.351234  [   64/150000]\n",
      "loss: 0.282986  [ 6464/150000]\n",
      "loss: 0.148447  [12864/150000]\n",
      "loss: 0.248153  [19264/150000]\n",
      "loss: 0.231883  [25664/150000]\n",
      "loss: 0.227387  [32064/150000]\n",
      "loss: 0.202279  [38464/150000]\n",
      "loss: 0.284582  [44864/150000]\n",
      "loss: 0.149374  [51264/150000]\n",
      "loss: 0.277824  [57664/150000]\n",
      "loss: 0.148711  [64064/150000]\n",
      "loss: 0.109924  [70464/150000]\n",
      "loss: 0.327780  [76864/150000]\n",
      "loss: 0.080768  [83264/150000]\n",
      "loss: 0.219992  [89664/150000]\n",
      "loss: 0.152806  [96064/150000]\n",
      "loss: 0.880489  [102464/150000]\n",
      "loss: 0.804903  [108864/150000]\n",
      "loss: 1.154043  [115264/150000]\n",
      "loss: 0.912716  [121664/150000]\n",
      "loss: 0.942214  [128064/150000]\n",
      "loss: 1.136114  [134464/150000]\n",
      "loss: 0.888712  [140864/150000]\n",
      "loss: 1.034924  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.814037 \n",
      "\n",
      "training epoch 24\n",
      "loss: 0.421000  [   64/150000]\n",
      "loss: 0.143066  [ 6464/150000]\n",
      "loss: 0.247126  [12864/150000]\n",
      "loss: 0.221497  [19264/150000]\n",
      "loss: 0.231937  [25664/150000]\n",
      "loss: 0.263890  [32064/150000]\n",
      "loss: 0.245730  [38464/150000]\n",
      "loss: 0.341932  [44864/150000]\n",
      "loss: 0.178072  [51264/150000]\n",
      "loss: 0.063792  [57664/150000]\n",
      "loss: 0.327045  [64064/150000]\n",
      "loss: 0.144084  [70464/150000]\n",
      "loss: 0.084613  [76864/150000]\n",
      "loss: 0.089074  [83264/150000]\n",
      "loss: 0.194321  [89664/150000]\n",
      "loss: 0.096022  [96064/150000]\n",
      "loss: 0.933780  [102464/150000]\n",
      "loss: 0.863810  [108864/150000]\n",
      "loss: 1.008183  [115264/150000]\n",
      "loss: 1.015358  [121664/150000]\n",
      "loss: 0.972575  [128064/150000]\n",
      "loss: 1.130890  [134464/150000]\n",
      "loss: 1.044491  [140864/150000]\n",
      "loss: 0.873538  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.789770 \n",
      "\n",
      "training epoch 25\n",
      "loss: 0.420844  [   64/150000]\n",
      "loss: 0.174479  [ 6464/150000]\n",
      "loss: 0.142523  [12864/150000]\n",
      "loss: 0.355774  [19264/150000]\n",
      "loss: 0.181736  [25664/150000]\n",
      "loss: 0.311111  [32064/150000]\n",
      "loss: 0.278701  [38464/150000]\n",
      "loss: 0.280200  [44864/150000]\n",
      "loss: 0.105691  [51264/150000]\n",
      "loss: 0.170495  [57664/150000]\n",
      "loss: 0.105208  [64064/150000]\n",
      "loss: 0.116127  [70464/150000]\n",
      "loss: 0.122470  [76864/150000]\n",
      "loss: 0.069634  [83264/150000]\n",
      "loss: 0.242197  [89664/150000]\n",
      "loss: 0.157449  [96064/150000]\n",
      "loss: 0.895817  [102464/150000]\n",
      "loss: 0.700430  [108864/150000]\n",
      "loss: 1.289067  [115264/150000]\n",
      "loss: 0.830149  [121664/150000]\n",
      "loss: 1.092517  [128064/150000]\n",
      "loss: 1.085369  [134464/150000]\n",
      "loss: 0.976475  [140864/150000]\n",
      "loss: 0.930186  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.798669 \n",
      "\n",
      "training epoch 26\n",
      "loss: 0.259736  [   64/150000]\n",
      "loss: 0.246542  [ 6464/150000]\n",
      "loss: 0.151938  [12864/150000]\n",
      "loss: 0.183037  [19264/150000]\n",
      "loss: 0.320499  [25664/150000]\n",
      "loss: 0.171632  [32064/150000]\n",
      "loss: 0.190025  [38464/150000]\n",
      "loss: 0.305053  [44864/150000]\n",
      "loss: 0.277833  [51264/150000]\n",
      "loss: 0.067451  [57664/150000]\n",
      "loss: 0.180182  [64064/150000]\n",
      "loss: 0.162672  [70464/150000]\n",
      "loss: 0.121050  [76864/150000]\n",
      "loss: 0.050738  [83264/150000]\n",
      "loss: 0.263636  [89664/150000]\n",
      "loss: 0.048358  [96064/150000]\n",
      "loss: 0.811336  [102464/150000]\n",
      "loss: 0.854588  [108864/150000]\n",
      "loss: 1.275562  [115264/150000]\n",
      "loss: 0.845936  [121664/150000]\n",
      "loss: 1.168714  [128064/150000]\n",
      "loss: 1.048451  [134464/150000]\n",
      "loss: 1.056385  [140864/150000]\n",
      "loss: 0.756167  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.768165 \n",
      "\n",
      "training epoch 27\n",
      "loss: 0.478438  [   64/150000]\n",
      "loss: 0.099358  [ 6464/150000]\n",
      "loss: 0.105666  [12864/150000]\n",
      "loss: 0.344024  [19264/150000]\n",
      "loss: 0.173714  [25664/150000]\n",
      "loss: 0.392548  [32064/150000]\n",
      "loss: 0.169780  [38464/150000]\n",
      "loss: 0.268044  [44864/150000]\n",
      "loss: 0.101398  [51264/150000]\n",
      "loss: 0.106576  [57664/150000]\n",
      "loss: 0.264202  [64064/150000]\n",
      "loss: 0.154936  [70464/150000]\n",
      "loss: 0.097257  [76864/150000]\n",
      "loss: 0.211668  [83264/150000]\n",
      "loss: 0.141584  [89664/150000]\n",
      "loss: 0.078204  [96064/150000]\n",
      "loss: 0.945570  [102464/150000]\n",
      "loss: 0.780004  [108864/150000]\n",
      "loss: 1.092446  [115264/150000]\n",
      "loss: 0.827002  [121664/150000]\n",
      "loss: 1.092941  [128064/150000]\n",
      "loss: 0.941089  [134464/150000]\n",
      "loss: 1.160879  [140864/150000]\n",
      "loss: 0.909363  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.815362 \n",
      "\n",
      "training epoch 28\n",
      "loss: 0.414826  [   64/150000]\n",
      "loss: 0.212604  [ 6464/150000]\n",
      "loss: 0.108544  [12864/150000]\n",
      "loss: 0.304885  [19264/150000]\n",
      "loss: 0.181968  [25664/150000]\n",
      "loss: 0.302010  [32064/150000]\n",
      "loss: 0.210635  [38464/150000]\n",
      "loss: 0.135004  [44864/150000]\n",
      "loss: 0.173524  [51264/150000]\n",
      "loss: 0.263006  [57664/150000]\n",
      "loss: 0.103151  [64064/150000]\n",
      "loss: 0.109676  [70464/150000]\n",
      "loss: 0.316009  [76864/150000]\n",
      "loss: 0.097375  [83264/150000]\n",
      "loss: 0.177007  [89664/150000]\n",
      "loss: 0.071348  [96064/150000]\n",
      "loss: 1.214596  [102464/150000]\n",
      "loss: 0.921813  [108864/150000]\n",
      "loss: 0.739067  [115264/150000]\n",
      "loss: 0.833792  [121664/150000]\n",
      "loss: 0.783157  [128064/150000]\n",
      "loss: 1.020035  [134464/150000]\n",
      "loss: 1.137527  [140864/150000]\n",
      "loss: 1.042737  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.809859 \n",
      "\n",
      "training epoch 29\n",
      "loss: 0.367296  [   64/150000]\n",
      "loss: 0.152383  [ 6464/150000]\n",
      "loss: 0.083512  [12864/150000]\n",
      "loss: 0.359437  [19264/150000]\n",
      "loss: 0.178597  [25664/150000]\n",
      "loss: 0.301069  [32064/150000]\n",
      "loss: 0.186203  [38464/150000]\n",
      "loss: 0.289625  [44864/150000]\n",
      "loss: 0.134419  [51264/150000]\n",
      "loss: 0.045902  [57664/150000]\n",
      "loss: 0.105498  [64064/150000]\n",
      "loss: 0.186694  [70464/150000]\n",
      "loss: 0.063624  [76864/150000]\n",
      "loss: 0.212340  [83264/150000]\n",
      "loss: 0.119793  [89664/150000]\n",
      "loss: 0.062903  [96064/150000]\n",
      "loss: 0.974590  [102464/150000]\n",
      "loss: 0.945260  [108864/150000]\n",
      "loss: 0.847901  [115264/150000]\n",
      "loss: 0.822263  [121664/150000]\n",
      "loss: 1.156136  [128064/150000]\n",
      "loss: 0.903428  [134464/150000]\n",
      "loss: 1.110196  [140864/150000]\n",
      "loss: 1.012473  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.807437 \n",
      "\n",
      "training epoch 30\n",
      "loss: 0.313749  [   64/150000]\n",
      "loss: 0.205232  [ 6464/150000]\n",
      "loss: 0.131510  [12864/150000]\n",
      "loss: 0.222268  [19264/150000]\n",
      "loss: 0.170310  [25664/150000]\n",
      "loss: 0.263833  [32064/150000]\n",
      "loss: 0.221108  [38464/150000]\n",
      "loss: 0.330229  [44864/150000]\n",
      "loss: 0.180350  [51264/150000]\n",
      "loss: 0.071137  [57664/150000]\n",
      "loss: 0.099599  [64064/150000]\n",
      "loss: 0.154240  [70464/150000]\n",
      "loss: 0.176185  [76864/150000]\n",
      "loss: 0.078977  [83264/150000]\n",
      "loss: 0.096767  [89664/150000]\n",
      "loss: 0.189154  [96064/150000]\n",
      "loss: 1.061403  [102464/150000]\n",
      "loss: 0.877478  [108864/150000]\n",
      "loss: 0.966719  [115264/150000]\n",
      "loss: 1.054487  [121664/150000]\n",
      "loss: 1.153099  [128064/150000]\n",
      "loss: 0.928140  [134464/150000]\n",
      "loss: 0.775902  [140864/150000]\n",
      "loss: 0.871407  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.777985 \n",
      "\n",
      "training epoch 31\n",
      "loss: 0.404341  [   64/150000]\n",
      "loss: 0.205896  [ 6464/150000]\n",
      "loss: 0.061912  [12864/150000]\n",
      "loss: 0.246435  [19264/150000]\n",
      "loss: 0.235075  [25664/150000]\n",
      "loss: 0.212513  [32064/150000]\n",
      "loss: 0.207378  [38464/150000]\n",
      "loss: 0.220096  [44864/150000]\n",
      "loss: 0.144314  [51264/150000]\n",
      "loss: 0.083249  [57664/150000]\n",
      "loss: 0.142133  [64064/150000]\n",
      "loss: 0.088496  [70464/150000]\n",
      "loss: 0.056828  [76864/150000]\n",
      "loss: 0.129265  [83264/150000]\n",
      "loss: 0.161654  [89664/150000]\n",
      "loss: 0.073153  [96064/150000]\n",
      "loss: 0.910883  [102464/150000]\n",
      "loss: 0.977154  [108864/150000]\n",
      "loss: 1.216564  [115264/150000]\n",
      "loss: 1.013867  [121664/150000]\n",
      "loss: 0.871997  [128064/150000]\n",
      "loss: 0.976226  [134464/150000]\n",
      "loss: 1.075958  [140864/150000]\n",
      "loss: 0.778855  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.797685 \n",
      "\n",
      "training epoch 32\n",
      "loss: 0.392739  [   64/150000]\n",
      "loss: 0.166631  [ 6464/150000]\n",
      "loss: 0.081241  [12864/150000]\n",
      "loss: 0.251697  [19264/150000]\n",
      "loss: 0.123664  [25664/150000]\n",
      "loss: 0.214920  [32064/150000]\n",
      "loss: 0.126394  [38464/150000]\n",
      "loss: 0.217498  [44864/150000]\n",
      "loss: 0.082344  [51264/150000]\n",
      "loss: 0.032839  [57664/150000]\n",
      "loss: 0.045543  [64064/150000]\n",
      "loss: 0.142363  [70464/150000]\n",
      "loss: 0.145966  [76864/150000]\n",
      "loss: 0.206802  [83264/150000]\n",
      "loss: 0.089570  [89664/150000]\n",
      "loss: 0.232903  [96064/150000]\n",
      "loss: 0.986533  [102464/150000]\n",
      "loss: 1.106754  [108864/150000]\n",
      "loss: 1.039261  [115264/150000]\n",
      "loss: 0.929988  [121664/150000]\n",
      "loss: 1.016355  [128064/150000]\n",
      "loss: 1.177084  [134464/150000]\n",
      "loss: 1.022342  [140864/150000]\n",
      "loss: 0.994412  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.786764 \n",
      "\n",
      "training epoch 33\n",
      "loss: 0.361643  [   64/150000]\n",
      "loss: 0.218336  [ 6464/150000]\n",
      "loss: 0.139525  [12864/150000]\n",
      "loss: 0.218537  [19264/150000]\n",
      "loss: 0.140819  [25664/150000]\n",
      "loss: 0.179190  [32064/150000]\n",
      "loss: 0.174458  [38464/150000]\n",
      "loss: 0.314860  [44864/150000]\n",
      "loss: 0.089619  [51264/150000]\n",
      "loss: 0.098675  [57664/150000]\n",
      "loss: 0.113191  [64064/150000]\n",
      "loss: 0.081826  [70464/150000]\n",
      "loss: 0.037610  [76864/150000]\n",
      "loss: 0.058697  [83264/150000]\n",
      "loss: 0.147607  [89664/150000]\n",
      "loss: 0.131824  [96064/150000]\n",
      "loss: 1.054113  [102464/150000]\n",
      "loss: 0.998918  [108864/150000]\n",
      "loss: 1.097478  [115264/150000]\n",
      "loss: 1.118130  [121664/150000]\n",
      "loss: 0.776762  [128064/150000]\n",
      "loss: 0.894190  [134464/150000]\n",
      "loss: 1.081863  [140864/150000]\n",
      "loss: 0.845048  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.783308 \n",
      "\n",
      "training epoch 34\n",
      "loss: 0.524091  [   64/150000]\n",
      "loss: 0.110801  [ 6464/150000]\n",
      "loss: 0.122254  [12864/150000]\n",
      "loss: 0.214947  [19264/150000]\n",
      "loss: 0.140587  [25664/150000]\n",
      "loss: 0.114642  [32064/150000]\n",
      "loss: 0.196515  [38464/150000]\n",
      "loss: 0.071189  [44864/150000]\n",
      "loss: 0.188206  [51264/150000]\n",
      "loss: 0.050317  [57664/150000]\n",
      "loss: 0.074014  [64064/150000]\n",
      "loss: 0.134021  [70464/150000]\n",
      "loss: 0.389007  [76864/150000]\n",
      "loss: 0.033908  [83264/150000]\n",
      "loss: 0.103772  [89664/150000]\n",
      "loss: 0.098017  [96064/150000]\n",
      "loss: 1.012744  [102464/150000]\n",
      "loss: 0.840751  [108864/150000]\n",
      "loss: 1.062371  [115264/150000]\n",
      "loss: 0.773603  [121664/150000]\n",
      "loss: 0.871547  [128064/150000]\n",
      "loss: 1.110750  [134464/150000]\n",
      "loss: 1.059395  [140864/150000]\n",
      "loss: 0.789165  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.786145 \n",
      "\n",
      "training epoch 35\n",
      "loss: 0.358350  [   64/150000]\n",
      "loss: 0.196927  [ 6464/150000]\n",
      "loss: 0.055928  [12864/150000]\n",
      "loss: 0.229537  [19264/150000]\n",
      "loss: 0.169451  [25664/150000]\n",
      "loss: 0.102116  [32064/150000]\n",
      "loss: 0.114498  [38464/150000]\n",
      "loss: 0.239781  [44864/150000]\n",
      "loss: 0.114784  [51264/150000]\n",
      "loss: 0.156294  [57664/150000]\n",
      "loss: 0.163085  [64064/150000]\n",
      "loss: 0.209715  [70464/150000]\n",
      "loss: 0.135598  [76864/150000]\n",
      "loss: 0.051772  [83264/150000]\n",
      "loss: 0.188972  [89664/150000]\n",
      "loss: 0.061498  [96064/150000]\n",
      "loss: 1.140675  [102464/150000]\n",
      "loss: 0.835973  [108864/150000]\n",
      "loss: 1.226367  [115264/150000]\n",
      "loss: 0.928842  [121664/150000]\n",
      "loss: 0.839632  [128064/150000]\n",
      "loss: 1.015759  [134464/150000]\n",
      "loss: 0.968147  [140864/150000]\n",
      "loss: 0.957729  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.769164 \n",
      "\n",
      "training epoch 36\n",
      "loss: 0.303967  [   64/150000]\n",
      "loss: 0.117372  [ 6464/150000]\n",
      "loss: 0.117268  [12864/150000]\n",
      "loss: 0.216231  [19264/150000]\n",
      "loss: 0.133022  [25664/150000]\n",
      "loss: 0.123400  [32064/150000]\n",
      "loss: 0.186915  [38464/150000]\n",
      "loss: 0.132097  [44864/150000]\n",
      "loss: 0.081172  [51264/150000]\n",
      "loss: 0.127090  [57664/150000]\n",
      "loss: 0.128665  [64064/150000]\n",
      "loss: 0.043663  [70464/150000]\n",
      "loss: 0.081736  [76864/150000]\n",
      "loss: 0.210465  [83264/150000]\n",
      "loss: 0.069684  [89664/150000]\n",
      "loss: 0.094035  [96064/150000]\n",
      "loss: 0.924164  [102464/150000]\n",
      "loss: 1.055772  [108864/150000]\n",
      "loss: 1.056023  [115264/150000]\n",
      "loss: 0.884591  [121664/150000]\n",
      "loss: 1.133917  [128064/150000]\n",
      "loss: 1.002091  [134464/150000]\n",
      "loss: 0.988754  [140864/150000]\n",
      "loss: 0.956254  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.758431 \n",
      "\n",
      "training epoch 37\n",
      "loss: 0.390916  [   64/150000]\n",
      "loss: 0.096669  [ 6464/150000]\n",
      "loss: 0.125819  [12864/150000]\n",
      "loss: 0.159006  [19264/150000]\n",
      "loss: 0.189530  [25664/150000]\n",
      "loss: 0.254054  [32064/150000]\n",
      "loss: 0.154204  [38464/150000]\n",
      "loss: 0.233323  [44864/150000]\n",
      "loss: 0.120721  [51264/150000]\n",
      "loss: 0.064425  [57664/150000]\n",
      "loss: 0.239772  [64064/150000]\n",
      "loss: 0.139966  [70464/150000]\n",
      "loss: 0.126444  [76864/150000]\n",
      "loss: 0.237531  [83264/150000]\n",
      "loss: 0.314400  [89664/150000]\n",
      "loss: 0.128474  [96064/150000]\n",
      "loss: 0.968432  [102464/150000]\n",
      "loss: 0.741000  [108864/150000]\n",
      "loss: 1.226819  [115264/150000]\n",
      "loss: 0.971687  [121664/150000]\n",
      "loss: 0.903179  [128064/150000]\n",
      "loss: 1.013326  [134464/150000]\n",
      "loss: 0.994485  [140864/150000]\n",
      "loss: 0.883097  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.763751 \n",
      "\n",
      "training epoch 38\n",
      "loss: 0.284528  [   64/150000]\n",
      "loss: 0.154163  [ 6464/150000]\n",
      "loss: 0.088035  [12864/150000]\n",
      "loss: 0.301351  [19264/150000]\n",
      "loss: 0.103709  [25664/150000]\n",
      "loss: 0.176079  [32064/150000]\n",
      "loss: 0.209343  [38464/150000]\n",
      "loss: 0.133056  [44864/150000]\n",
      "loss: 0.052847  [51264/150000]\n",
      "loss: 0.051284  [57664/150000]\n",
      "loss: 0.164407  [64064/150000]\n",
      "loss: 0.053474  [70464/150000]\n",
      "loss: 0.255183  [76864/150000]\n",
      "loss: 0.240193  [83264/150000]\n",
      "loss: 0.042015  [89664/150000]\n",
      "loss: 0.056375  [96064/150000]\n",
      "loss: 1.046950  [102464/150000]\n",
      "loss: 1.078499  [108864/150000]\n",
      "loss: 0.949901  [115264/150000]\n",
      "loss: 0.758040  [121664/150000]\n",
      "loss: 0.958926  [128064/150000]\n",
      "loss: 1.009312  [134464/150000]\n",
      "loss: 0.971885  [140864/150000]\n",
      "loss: 0.798793  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.781654 \n",
      "\n",
      "training epoch 39\n",
      "loss: 0.326726  [   64/150000]\n",
      "loss: 0.101104  [ 6464/150000]\n",
      "loss: 0.110257  [12864/150000]\n",
      "loss: 0.103635  [19264/150000]\n",
      "loss: 0.099857  [25664/150000]\n",
      "loss: 0.219999  [32064/150000]\n",
      "loss: 0.043604  [38464/150000]\n",
      "loss: 0.241638  [44864/150000]\n",
      "loss: 0.054861  [51264/150000]\n",
      "loss: 0.065448  [57664/150000]\n",
      "loss: 0.105378  [64064/150000]\n",
      "loss: 0.097971  [70464/150000]\n",
      "loss: 0.065063  [76864/150000]\n",
      "loss: 0.119823  [83264/150000]\n",
      "loss: 0.053814  [89664/150000]\n",
      "loss: 0.078151  [96064/150000]\n",
      "loss: 1.020372  [102464/150000]\n",
      "loss: 0.877899  [108864/150000]\n",
      "loss: 0.958588  [115264/150000]\n",
      "loss: 1.244208  [121664/150000]\n",
      "loss: 0.876698  [128064/150000]\n",
      "loss: 1.139252  [134464/150000]\n",
      "loss: 0.920336  [140864/150000]\n",
      "loss: 0.969796  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.804895 \n",
      "\n",
      "training epoch 40\n",
      "loss: 0.295647  [   64/150000]\n",
      "loss: 0.113743  [ 6464/150000]\n",
      "loss: 0.114462  [12864/150000]\n",
      "loss: 0.206020  [19264/150000]\n",
      "loss: 0.108581  [25664/150000]\n",
      "loss: 0.292089  [32064/150000]\n",
      "loss: 0.129140  [38464/150000]\n",
      "loss: 0.292257  [44864/150000]\n",
      "loss: 0.114610  [51264/150000]\n",
      "loss: 0.208126  [57664/150000]\n",
      "loss: 0.073574  [64064/150000]\n",
      "loss: 0.048237  [70464/150000]\n",
      "loss: 0.319265  [76864/150000]\n",
      "loss: 0.103328  [83264/150000]\n",
      "loss: 0.213916  [89664/150000]\n",
      "loss: 0.137548  [96064/150000]\n",
      "loss: 0.958722  [102464/150000]\n",
      "loss: 1.079915  [108864/150000]\n",
      "loss: 1.050218  [115264/150000]\n",
      "loss: 0.803106  [121664/150000]\n",
      "loss: 0.863581  [128064/150000]\n",
      "loss: 1.290398  [134464/150000]\n",
      "loss: 0.846504  [140864/150000]\n",
      "loss: 0.847926  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.846546 \n",
      "\n",
      "training epoch 41\n",
      "loss: 0.372480  [   64/150000]\n",
      "loss: 0.202883  [ 6464/150000]\n",
      "loss: 0.102246  [12864/150000]\n",
      "loss: 0.173210  [19264/150000]\n",
      "loss: 0.191112  [25664/150000]\n",
      "loss: 0.244441  [32064/150000]\n",
      "loss: 0.135361  [38464/150000]\n",
      "loss: 0.054264  [44864/150000]\n",
      "loss: 0.057962  [51264/150000]\n",
      "loss: 0.081850  [57664/150000]\n",
      "loss: 0.056346  [64064/150000]\n",
      "loss: 0.058548  [70464/150000]\n",
      "loss: 0.122855  [76864/150000]\n",
      "loss: 0.013464  [83264/150000]\n",
      "loss: 0.044888  [89664/150000]\n",
      "loss: 0.057997  [96064/150000]\n",
      "loss: 0.880663  [102464/150000]\n",
      "loss: 0.955551  [108864/150000]\n",
      "loss: 1.045502  [115264/150000]\n",
      "loss: 0.839312  [121664/150000]\n",
      "loss: 0.959446  [128064/150000]\n",
      "loss: 1.234979  [134464/150000]\n",
      "loss: 0.823805  [140864/150000]\n",
      "loss: 0.779911  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.787765 \n",
      "\n",
      "training epoch 42\n",
      "loss: 0.310778  [   64/150000]\n",
      "loss: 0.079652  [ 6464/150000]\n",
      "loss: 0.117600  [12864/150000]\n",
      "loss: 0.096647  [19264/150000]\n",
      "loss: 0.082969  [25664/150000]\n",
      "loss: 0.138355  [32064/150000]\n",
      "loss: 0.137060  [38464/150000]\n",
      "loss: 0.153982  [44864/150000]\n",
      "loss: 0.091239  [51264/150000]\n",
      "loss: 0.041421  [57664/150000]\n",
      "loss: 0.162111  [64064/150000]\n",
      "loss: 0.105173  [70464/150000]\n",
      "loss: 0.408152  [76864/150000]\n",
      "loss: 0.081671  [83264/150000]\n",
      "loss: 0.105701  [89664/150000]\n",
      "loss: 0.046955  [96064/150000]\n",
      "loss: 0.916341  [102464/150000]\n",
      "loss: 0.872712  [108864/150000]\n",
      "loss: 1.010816  [115264/150000]\n",
      "loss: 0.908379  [121664/150000]\n",
      "loss: 0.969991  [128064/150000]\n",
      "loss: 0.941495  [134464/150000]\n",
      "loss: 0.937462  [140864/150000]\n",
      "loss: 0.909837  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.797991 \n",
      "\n",
      "training epoch 43\n",
      "loss: 0.248437  [   64/150000]\n",
      "loss: 0.177690  [ 6464/150000]\n",
      "loss: 0.154951  [12864/150000]\n",
      "loss: 0.212882  [19264/150000]\n",
      "loss: 0.171508  [25664/150000]\n",
      "loss: 0.157653  [32064/150000]\n",
      "loss: 0.161934  [38464/150000]\n",
      "loss: 0.209172  [44864/150000]\n",
      "loss: 0.058349  [51264/150000]\n",
      "loss: 0.032822  [57664/150000]\n",
      "loss: 0.087066  [64064/150000]\n",
      "loss: 0.094326  [70464/150000]\n",
      "loss: 0.086441  [76864/150000]\n",
      "loss: 0.292066  [83264/150000]\n",
      "loss: 0.265270  [89664/150000]\n",
      "loss: 0.040180  [96064/150000]\n",
      "loss: 0.964114  [102464/150000]\n",
      "loss: 0.984119  [108864/150000]\n",
      "loss: 1.066449  [115264/150000]\n",
      "loss: 1.014960  [121664/150000]\n",
      "loss: 0.969202  [128064/150000]\n",
      "loss: 0.754260  [134464/150000]\n",
      "loss: 0.978448  [140864/150000]\n",
      "loss: 0.901910  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.803293 \n",
      "\n",
      "training epoch 44\n",
      "loss: 0.404099  [   64/150000]\n",
      "loss: 0.114105  [ 6464/150000]\n",
      "loss: 0.049202  [12864/150000]\n",
      "loss: 0.164527  [19264/150000]\n",
      "loss: 0.215110  [25664/150000]\n",
      "loss: 0.254031  [32064/150000]\n",
      "loss: 0.214961  [38464/150000]\n",
      "loss: 0.162709  [44864/150000]\n",
      "loss: 0.067491  [51264/150000]\n",
      "loss: 0.017357  [57664/150000]\n",
      "loss: 0.185707  [64064/150000]\n",
      "loss: 0.046980  [70464/150000]\n",
      "loss: 0.038052  [76864/150000]\n",
      "loss: 0.068455  [83264/150000]\n",
      "loss: 0.261975  [89664/150000]\n",
      "loss: 0.088542  [96064/150000]\n",
      "loss: 1.024986  [102464/150000]\n",
      "loss: 0.796987  [108864/150000]\n",
      "loss: 1.102829  [115264/150000]\n",
      "loss: 0.861403  [121664/150000]\n",
      "loss: 0.891451  [128064/150000]\n",
      "loss: 1.119083  [134464/150000]\n",
      "loss: 0.988044  [140864/150000]\n",
      "loss: 0.871967  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.846679 \n",
      "\n",
      "training epoch 45\n",
      "loss: 0.434332  [   64/150000]\n",
      "loss: 0.194243  [ 6464/150000]\n",
      "loss: 0.106430  [12864/150000]\n",
      "loss: 0.200516  [19264/150000]\n",
      "loss: 0.144910  [25664/150000]\n",
      "loss: 0.152111  [32064/150000]\n",
      "loss: 0.167733  [38464/150000]\n",
      "loss: 0.079158  [44864/150000]\n",
      "loss: 0.089439  [51264/150000]\n",
      "loss: 0.166171  [57664/150000]\n",
      "loss: 0.029791  [64064/150000]\n",
      "loss: 0.043122  [70464/150000]\n",
      "loss: 0.052718  [76864/150000]\n",
      "loss: 0.106386  [83264/150000]\n",
      "loss: 0.202639  [89664/150000]\n",
      "loss: 0.259044  [96064/150000]\n",
      "loss: 0.978640  [102464/150000]\n",
      "loss: 1.153028  [108864/150000]\n",
      "loss: 1.092498  [115264/150000]\n",
      "loss: 0.752921  [121664/150000]\n",
      "loss: 0.954676  [128064/150000]\n",
      "loss: 0.962935  [134464/150000]\n",
      "loss: 1.128116  [140864/150000]\n",
      "loss: 0.865958  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.794783 \n",
      "\n",
      "training epoch 46\n",
      "loss: 0.258637  [   64/150000]\n",
      "loss: 0.123308  [ 6464/150000]\n",
      "loss: 0.130302  [12864/150000]\n",
      "loss: 0.097263  [19264/150000]\n",
      "loss: 0.175856  [25664/150000]\n",
      "loss: 0.172795  [32064/150000]\n",
      "loss: 0.312285  [38464/150000]\n",
      "loss: 0.073717  [44864/150000]\n",
      "loss: 0.049173  [51264/150000]\n",
      "loss: 0.135467  [57664/150000]\n",
      "loss: 0.108385  [64064/150000]\n",
      "loss: 0.162486  [70464/150000]\n",
      "loss: 0.098205  [76864/150000]\n",
      "loss: 0.100856  [83264/150000]\n",
      "loss: 0.138320  [89664/150000]\n",
      "loss: 0.055925  [96064/150000]\n",
      "loss: 0.913620  [102464/150000]\n",
      "loss: 0.749512  [108864/150000]\n",
      "loss: 1.038301  [115264/150000]\n",
      "loss: 0.970527  [121664/150000]\n",
      "loss: 0.769086  [128064/150000]\n",
      "loss: 1.121334  [134464/150000]\n",
      "loss: 0.972298  [140864/150000]\n",
      "loss: 0.910111  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.785560 \n",
      "\n",
      "training epoch 47\n",
      "loss: 0.350709  [   64/150000]\n",
      "loss: 0.067365  [ 6464/150000]\n",
      "loss: 0.045089  [12864/150000]\n",
      "loss: 0.291715  [19264/150000]\n",
      "loss: 0.084678  [25664/150000]\n",
      "loss: 0.211587  [32064/150000]\n",
      "loss: 0.145780  [38464/150000]\n",
      "loss: 0.051609  [44864/150000]\n",
      "loss: 0.040575  [51264/150000]\n",
      "loss: 0.046003  [57664/150000]\n",
      "loss: 0.083983  [64064/150000]\n",
      "loss: 0.091532  [70464/150000]\n",
      "loss: 0.086196  [76864/150000]\n",
      "loss: 0.025433  [83264/150000]\n",
      "loss: 0.066687  [89664/150000]\n",
      "loss: 0.111956  [96064/150000]\n",
      "loss: 0.871252  [102464/150000]\n",
      "loss: 0.844740  [108864/150000]\n",
      "loss: 1.137287  [115264/150000]\n",
      "loss: 0.862849  [121664/150000]\n",
      "loss: 0.916394  [128064/150000]\n",
      "loss: 0.852153  [134464/150000]\n",
      "loss: 1.105551  [140864/150000]\n",
      "loss: 0.884762  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.787930 \n",
      "\n",
      "training epoch 48\n",
      "loss: 0.286088  [   64/150000]\n",
      "loss: 0.120829  [ 6464/150000]\n",
      "loss: 0.062814  [12864/150000]\n",
      "loss: 0.232784  [19264/150000]\n",
      "loss: 0.224903  [25664/150000]\n",
      "loss: 0.189025  [32064/150000]\n",
      "loss: 0.059861  [38464/150000]\n",
      "loss: 0.202632  [44864/150000]\n",
      "loss: 0.173999  [51264/150000]\n",
      "loss: 0.217766  [57664/150000]\n",
      "loss: 0.194187  [64064/150000]\n",
      "loss: 0.079518  [70464/150000]\n",
      "loss: 0.169519  [76864/150000]\n",
      "loss: 0.133389  [83264/150000]\n",
      "loss: 0.243011  [89664/150000]\n",
      "loss: 0.035301  [96064/150000]\n",
      "loss: 1.026486  [102464/150000]\n",
      "loss: 0.927336  [108864/150000]\n",
      "loss: 0.822560  [115264/150000]\n",
      "loss: 1.071411  [121664/150000]\n",
      "loss: 0.921835  [128064/150000]\n",
      "loss: 1.036427  [134464/150000]\n",
      "loss: 1.116487  [140864/150000]\n",
      "loss: 0.694529  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.806455 \n",
      "\n",
      "training epoch 49\n",
      "loss: 0.227715  [   64/150000]\n",
      "loss: 0.137611  [ 6464/150000]\n",
      "loss: 0.020125  [12864/150000]\n",
      "loss: 0.223751  [19264/150000]\n",
      "loss: 0.104818  [25664/150000]\n",
      "loss: 0.245304  [32064/150000]\n",
      "loss: 0.109579  [38464/150000]\n",
      "loss: 0.217452  [44864/150000]\n",
      "loss: 0.045221  [51264/150000]\n",
      "loss: 0.053972  [57664/150000]\n",
      "loss: 0.087556  [64064/150000]\n",
      "loss: 0.122633  [70464/150000]\n",
      "loss: 0.021057  [76864/150000]\n",
      "loss: 0.085089  [83264/150000]\n",
      "loss: 0.204488  [89664/150000]\n",
      "loss: 0.047939  [96064/150000]\n",
      "loss: 1.261241  [102464/150000]\n",
      "loss: 0.984958  [108864/150000]\n",
      "loss: 0.921081  [115264/150000]\n",
      "loss: 0.708795  [121664/150000]\n",
      "loss: 1.056727  [128064/150000]\n",
      "loss: 0.947166  [134464/150000]\n",
      "loss: 0.821495  [140864/150000]\n",
      "loss: 1.030514  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.753519 \n",
      "\n",
      "training epoch 50\n",
      "loss: 0.292913  [   64/150000]\n",
      "loss: 0.253899  [ 6464/150000]\n",
      "loss: 0.034419  [12864/150000]\n",
      "loss: 0.139462  [19264/150000]\n",
      "loss: 0.092081  [25664/150000]\n",
      "loss: 0.068373  [32064/150000]\n",
      "loss: 0.179868  [38464/150000]\n",
      "loss: 0.052406  [44864/150000]\n",
      "loss: 0.060558  [51264/150000]\n",
      "loss: 0.143413  [57664/150000]\n",
      "loss: 0.107988  [64064/150000]\n",
      "loss: 0.230943  [70464/150000]\n",
      "loss: 0.209782  [76864/150000]\n",
      "loss: 0.056897  [83264/150000]\n",
      "loss: 0.188359  [89664/150000]\n",
      "loss: 0.239336  [96064/150000]\n",
      "loss: 0.940953  [102464/150000]\n",
      "loss: 1.023687  [108864/150000]\n",
      "loss: 1.011704  [115264/150000]\n",
      "loss: 0.991966  [121664/150000]\n",
      "loss: 0.997904  [128064/150000]\n",
      "loss: 0.986986  [134464/150000]\n",
      "loss: 1.145790  [140864/150000]\n",
      "loss: 0.858932  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.791932 \n",
      "\n",
      "training epoch 51\n",
      "loss: 0.193469  [   64/150000]\n",
      "loss: 0.145834  [ 6464/150000]\n",
      "loss: 0.086591  [12864/150000]\n",
      "loss: 0.138250  [19264/150000]\n",
      "loss: 0.105558  [25664/150000]\n",
      "loss: 0.100125  [32064/150000]\n",
      "loss: 0.049292  [38464/150000]\n",
      "loss: 0.243852  [44864/150000]\n",
      "loss: 0.122557  [51264/150000]\n",
      "loss: 0.101059  [57664/150000]\n",
      "loss: 0.268434  [64064/150000]\n",
      "loss: 0.140960  [70464/150000]\n",
      "loss: 0.134809  [76864/150000]\n",
      "loss: 0.088015  [83264/150000]\n",
      "loss: 0.150071  [89664/150000]\n",
      "loss: 0.023634  [96064/150000]\n",
      "loss: 1.008427  [102464/150000]\n",
      "loss: 0.932342  [108864/150000]\n",
      "loss: 0.819434  [115264/150000]\n",
      "loss: 1.016407  [121664/150000]\n",
      "loss: 0.990240  [128064/150000]\n",
      "loss: 0.994767  [134464/150000]\n",
      "loss: 1.039942  [140864/150000]\n",
      "loss: 0.909020  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.789075 \n",
      "\n",
      "training epoch 52\n",
      "loss: 0.182790  [   64/150000]\n",
      "loss: 0.081521  [ 6464/150000]\n",
      "loss: 0.117782  [12864/150000]\n",
      "loss: 0.219684  [19264/150000]\n",
      "loss: 0.062381  [25664/150000]\n",
      "loss: 0.064360  [32064/150000]\n",
      "loss: 0.150957  [38464/150000]\n",
      "loss: 0.134697  [44864/150000]\n",
      "loss: 0.096070  [51264/150000]\n",
      "loss: 0.027751  [57664/150000]\n",
      "loss: 0.067101  [64064/150000]\n",
      "loss: 0.069919  [70464/150000]\n",
      "loss: 0.043024  [76864/150000]\n",
      "loss: 0.030193  [83264/150000]\n",
      "loss: 0.051115  [89664/150000]\n",
      "loss: 0.032402  [96064/150000]\n",
      "loss: 1.030373  [102464/150000]\n",
      "loss: 0.817786  [108864/150000]\n",
      "loss: 1.063631  [115264/150000]\n",
      "loss: 0.870293  [121664/150000]\n",
      "loss: 0.679933  [128064/150000]\n",
      "loss: 0.973731  [134464/150000]\n",
      "loss: 1.077859  [140864/150000]\n",
      "loss: 0.740216  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.783025 \n",
      "\n",
      "training epoch 53\n",
      "loss: 0.257187  [   64/150000]\n",
      "loss: 0.060177  [ 6464/150000]\n",
      "loss: 0.120945  [12864/150000]\n",
      "loss: 0.055628  [19264/150000]\n",
      "loss: 0.117437  [25664/150000]\n",
      "loss: 0.163365  [32064/150000]\n",
      "loss: 0.071175  [38464/150000]\n",
      "loss: 0.087861  [44864/150000]\n",
      "loss: 0.069135  [51264/150000]\n",
      "loss: 0.071951  [57664/150000]\n",
      "loss: 0.063071  [64064/150000]\n",
      "loss: 0.154110  [70464/150000]\n",
      "loss: 0.133564  [76864/150000]\n",
      "loss: 0.292214  [83264/150000]\n",
      "loss: 0.205810  [89664/150000]\n",
      "loss: 0.063572  [96064/150000]\n",
      "loss: 1.035463  [102464/150000]\n",
      "loss: 0.948266  [108864/150000]\n",
      "loss: 0.894601  [115264/150000]\n",
      "loss: 1.023881  [121664/150000]\n",
      "loss: 0.979978  [128064/150000]\n",
      "loss: 1.110626  [134464/150000]\n",
      "loss: 1.180687  [140864/150000]\n",
      "loss: 0.762610  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.807290 \n",
      "\n",
      "training epoch 54\n",
      "loss: 0.341946  [   64/150000]\n",
      "loss: 0.152623  [ 6464/150000]\n",
      "loss: 0.077078  [12864/150000]\n",
      "loss: 0.091680  [19264/150000]\n",
      "loss: 0.094874  [25664/150000]\n",
      "loss: 0.262706  [32064/150000]\n",
      "loss: 0.128342  [38464/150000]\n",
      "loss: 0.100487  [44864/150000]\n",
      "loss: 0.046125  [51264/150000]\n",
      "loss: 0.054605  [57664/150000]\n",
      "loss: 0.231597  [64064/150000]\n",
      "loss: 0.073887  [70464/150000]\n",
      "loss: 0.104591  [76864/150000]\n",
      "loss: 0.069034  [83264/150000]\n",
      "loss: 0.108827  [89664/150000]\n",
      "loss: 0.125978  [96064/150000]\n",
      "loss: 0.794329  [102464/150000]\n",
      "loss: 0.827436  [108864/150000]\n",
      "loss: 0.882462  [115264/150000]\n",
      "loss: 0.613203  [121664/150000]\n",
      "loss: 0.605857  [128064/150000]\n",
      "loss: 0.927768  [134464/150000]\n",
      "loss: 1.089103  [140864/150000]\n",
      "loss: 0.992765  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.784911 \n",
      "\n",
      "training epoch 55\n",
      "loss: 0.274629  [   64/150000]\n",
      "loss: 0.059794  [ 6464/150000]\n",
      "loss: 0.065522  [12864/150000]\n",
      "loss: 0.095487  [19264/150000]\n",
      "loss: 0.288522  [25664/150000]\n",
      "loss: 0.040169  [32064/150000]\n",
      "loss: 0.082152  [38464/150000]\n",
      "loss: 0.066385  [44864/150000]\n",
      "loss: 0.073971  [51264/150000]\n",
      "loss: 0.083157  [57664/150000]\n",
      "loss: 0.076364  [64064/150000]\n",
      "loss: 0.135121  [70464/150000]\n",
      "loss: 0.078631  [76864/150000]\n",
      "loss: 0.058204  [83264/150000]\n",
      "loss: 0.074572  [89664/150000]\n",
      "loss: 0.143418  [96064/150000]\n",
      "loss: 0.876996  [102464/150000]\n",
      "loss: 0.867998  [108864/150000]\n",
      "loss: 1.039464  [115264/150000]\n",
      "loss: 0.818447  [121664/150000]\n",
      "loss: 0.785167  [128064/150000]\n",
      "loss: 1.031315  [134464/150000]\n",
      "loss: 0.836195  [140864/150000]\n",
      "loss: 0.854189  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.779248 \n",
      "\n",
      "training epoch 56\n",
      "loss: 0.234779  [   64/150000]\n",
      "loss: 0.089390  [ 6464/150000]\n",
      "loss: 0.052240  [12864/150000]\n",
      "loss: 0.243677  [19264/150000]\n",
      "loss: 0.097033  [25664/150000]\n",
      "loss: 0.141569  [32064/150000]\n",
      "loss: 0.088832  [38464/150000]\n",
      "loss: 0.102720  [44864/150000]\n",
      "loss: 0.076857  [51264/150000]\n",
      "loss: 0.026473  [57664/150000]\n",
      "loss: 0.082016  [64064/150000]\n",
      "loss: 0.191684  [70464/150000]\n",
      "loss: 0.031040  [76864/150000]\n",
      "loss: 0.085653  [83264/150000]\n",
      "loss: 0.079044  [89664/150000]\n",
      "loss: 0.052891  [96064/150000]\n",
      "loss: 1.068889  [102464/150000]\n",
      "loss: 0.903298  [108864/150000]\n",
      "loss: 1.212685  [115264/150000]\n",
      "loss: 0.734782  [121664/150000]\n",
      "loss: 0.840143  [128064/150000]\n",
      "loss: 0.974156  [134464/150000]\n",
      "loss: 0.994918  [140864/150000]\n",
      "loss: 0.819840  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.742762 \n",
      "\n",
      "training epoch 57\n",
      "loss: 0.363621  [   64/150000]\n",
      "loss: 0.141318  [ 6464/150000]\n",
      "loss: 0.130875  [12864/150000]\n",
      "loss: 0.129571  [19264/150000]\n",
      "loss: 0.060647  [25664/150000]\n",
      "loss: 0.209346  [32064/150000]\n",
      "loss: 0.114294  [38464/150000]\n",
      "loss: 0.107692  [44864/150000]\n",
      "loss: 0.056593  [51264/150000]\n",
      "loss: 0.047272  [57664/150000]\n",
      "loss: 0.144498  [64064/150000]\n",
      "loss: 0.101517  [70464/150000]\n",
      "loss: 0.067751  [76864/150000]\n",
      "loss: 0.068280  [83264/150000]\n",
      "loss: 0.187627  [89664/150000]\n",
      "loss: 0.037674  [96064/150000]\n",
      "loss: 0.903004  [102464/150000]\n",
      "loss: 0.878035  [108864/150000]\n",
      "loss: 0.933527  [115264/150000]\n",
      "loss: 0.930480  [121664/150000]\n",
      "loss: 0.762076  [128064/150000]\n",
      "loss: 1.169148  [134464/150000]\n",
      "loss: 1.060209  [140864/150000]\n",
      "loss: 0.955222  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.815974 \n",
      "\n",
      "training epoch 58\n",
      "loss: 0.393677  [   64/150000]\n",
      "loss: 0.152592  [ 6464/150000]\n",
      "loss: 0.057020  [12864/150000]\n",
      "loss: 0.157612  [19264/150000]\n",
      "loss: 0.113650  [25664/150000]\n",
      "loss: 0.077195  [32064/150000]\n",
      "loss: 0.078531  [38464/150000]\n",
      "loss: 0.120761  [44864/150000]\n",
      "loss: 0.062448  [51264/150000]\n",
      "loss: 0.030687  [57664/150000]\n",
      "loss: 0.184619  [64064/150000]\n",
      "loss: 0.074433  [70464/150000]\n",
      "loss: 0.127389  [76864/150000]\n",
      "loss: 0.121696  [83264/150000]\n",
      "loss: 0.188321  [89664/150000]\n",
      "loss: 0.039296  [96064/150000]\n",
      "loss: 0.894840  [102464/150000]\n",
      "loss: 0.858143  [108864/150000]\n",
      "loss: 1.187199  [115264/150000]\n",
      "loss: 0.867492  [121664/150000]\n",
      "loss: 0.966633  [128064/150000]\n",
      "loss: 1.036081  [134464/150000]\n",
      "loss: 1.093316  [140864/150000]\n",
      "loss: 0.781333  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.794640 \n",
      "\n",
      "training epoch 59\n",
      "loss: 0.272622  [   64/150000]\n",
      "loss: 0.117601  [ 6464/150000]\n",
      "loss: 0.045396  [12864/150000]\n",
      "loss: 0.216048  [19264/150000]\n",
      "loss: 0.132569  [25664/150000]\n",
      "loss: 0.377828  [32064/150000]\n",
      "loss: 0.102382  [38464/150000]\n",
      "loss: 0.077615  [44864/150000]\n",
      "loss: 0.118818  [51264/150000]\n",
      "loss: 0.081811  [57664/150000]\n",
      "loss: 0.118384  [64064/150000]\n",
      "loss: 0.191527  [70464/150000]\n",
      "loss: 0.070252  [76864/150000]\n",
      "loss: 0.073847  [83264/150000]\n",
      "loss: 0.013223  [89664/150000]\n",
      "loss: 0.065374  [96064/150000]\n",
      "loss: 0.918041  [102464/150000]\n",
      "loss: 0.873164  [108864/150000]\n",
      "loss: 0.895885  [115264/150000]\n",
      "loss: 1.023342  [121664/150000]\n",
      "loss: 0.887316  [128064/150000]\n",
      "loss: 1.041935  [134464/150000]\n",
      "loss: 1.016284  [140864/150000]\n",
      "loss: 0.918462  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.795182 \n",
      "\n",
      "training epoch 60\n",
      "loss: 0.316002  [   64/150000]\n",
      "loss: 0.072827  [ 6464/150000]\n",
      "loss: 0.141857  [12864/150000]\n",
      "loss: 0.195731  [19264/150000]\n",
      "loss: 0.153243  [25664/150000]\n",
      "loss: 0.220146  [32064/150000]\n",
      "loss: 0.074106  [38464/150000]\n",
      "loss: 0.039795  [44864/150000]\n",
      "loss: 0.040487  [51264/150000]\n",
      "loss: 0.014478  [57664/150000]\n",
      "loss: 0.128901  [64064/150000]\n",
      "loss: 0.131061  [70464/150000]\n",
      "loss: 0.074476  [76864/150000]\n",
      "loss: 0.093840  [83264/150000]\n",
      "loss: 0.070510  [89664/150000]\n",
      "loss: 0.111223  [96064/150000]\n",
      "loss: 1.098579  [102464/150000]\n",
      "loss: 1.105865  [108864/150000]\n",
      "loss: 0.949976  [115264/150000]\n",
      "loss: 0.744259  [121664/150000]\n",
      "loss: 1.016739  [128064/150000]\n",
      "loss: 1.077420  [134464/150000]\n",
      "loss: 0.979523  [140864/150000]\n",
      "loss: 0.862859  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.797769 \n",
      "\n",
      "training epoch 61\n",
      "loss: 0.229779  [   64/150000]\n",
      "loss: 0.059686  [ 6464/150000]\n",
      "loss: 0.111759  [12864/150000]\n",
      "loss: 0.121928  [19264/150000]\n",
      "loss: 0.113464  [25664/150000]\n",
      "loss: 0.182952  [32064/150000]\n",
      "loss: 0.118379  [38464/150000]\n",
      "loss: 0.117428  [44864/150000]\n",
      "loss: 0.044730  [51264/150000]\n",
      "loss: 0.035173  [57664/150000]\n",
      "loss: 0.022861  [64064/150000]\n",
      "loss: 0.218610  [70464/150000]\n",
      "loss: 0.087621  [76864/150000]\n",
      "loss: 0.127230  [83264/150000]\n",
      "loss: 0.042134  [89664/150000]\n",
      "loss: 0.207168  [96064/150000]\n",
      "loss: 0.976345  [102464/150000]\n",
      "loss: 0.977986  [108864/150000]\n",
      "loss: 1.143379  [115264/150000]\n",
      "loss: 0.798260  [121664/150000]\n",
      "loss: 0.831386  [128064/150000]\n",
      "loss: 1.088048  [134464/150000]\n",
      "loss: 1.074898  [140864/150000]\n",
      "loss: 0.831549  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.834912 \n",
      "\n",
      "training epoch 62\n",
      "loss: 0.317106  [   64/150000]\n",
      "loss: 0.170035  [ 6464/150000]\n",
      "loss: 0.061165  [12864/150000]\n",
      "loss: 0.146417  [19264/150000]\n",
      "loss: 0.113479  [25664/150000]\n",
      "loss: 0.242470  [32064/150000]\n",
      "loss: 0.269766  [38464/150000]\n",
      "loss: 0.197763  [44864/150000]\n",
      "loss: 0.036163  [51264/150000]\n",
      "loss: 0.051341  [57664/150000]\n",
      "loss: 0.059185  [64064/150000]\n",
      "loss: 0.061287  [70464/150000]\n",
      "loss: 0.053450  [76864/150000]\n",
      "loss: 0.211523  [83264/150000]\n",
      "loss: 0.047025  [89664/150000]\n",
      "loss: 0.100478  [96064/150000]\n",
      "loss: 0.961207  [102464/150000]\n",
      "loss: 0.964541  [108864/150000]\n",
      "loss: 1.148348  [115264/150000]\n",
      "loss: 0.920742  [121664/150000]\n",
      "loss: 0.736688  [128064/150000]\n",
      "loss: 1.053382  [134464/150000]\n",
      "loss: 1.018508  [140864/150000]\n",
      "loss: 0.888277  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.765564 \n",
      "\n",
      "training epoch 63\n",
      "loss: 0.239527  [   64/150000]\n",
      "loss: 0.119635  [ 6464/150000]\n",
      "loss: 0.027773  [12864/150000]\n",
      "loss: 0.195388  [19264/150000]\n",
      "loss: 0.157149  [25664/150000]\n",
      "loss: 0.183186  [32064/150000]\n",
      "loss: 0.075043  [38464/150000]\n",
      "loss: 0.136397  [44864/150000]\n",
      "loss: 0.035290  [51264/150000]\n",
      "loss: 0.019366  [57664/150000]\n",
      "loss: 0.034461  [64064/150000]\n",
      "loss: 0.038061  [70464/150000]\n",
      "loss: 0.082525  [76864/150000]\n",
      "loss: 0.130706  [83264/150000]\n",
      "loss: 0.033500  [89664/150000]\n",
      "loss: 0.037776  [96064/150000]\n",
      "loss: 0.853896  [102464/150000]\n",
      "loss: 0.901837  [108864/150000]\n",
      "loss: 0.671201  [115264/150000]\n",
      "loss: 0.732965  [121664/150000]\n",
      "loss: 0.812315  [128064/150000]\n",
      "loss: 1.045252  [134464/150000]\n",
      "loss: 1.074244  [140864/150000]\n",
      "loss: 0.949716  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.777434 \n",
      "\n",
      "training epoch 64\n",
      "loss: 0.332839  [   64/150000]\n",
      "loss: 0.085566  [ 6464/150000]\n",
      "loss: 0.075741  [12864/150000]\n",
      "loss: 0.171091  [19264/150000]\n",
      "loss: 0.095565  [25664/150000]\n",
      "loss: 0.299946  [32064/150000]\n",
      "loss: 0.220768  [38464/150000]\n",
      "loss: 0.076223  [44864/150000]\n",
      "loss: 0.046194  [51264/150000]\n",
      "loss: 0.268855  [57664/150000]\n",
      "loss: 0.102976  [64064/150000]\n",
      "loss: 0.063694  [70464/150000]\n",
      "loss: 0.075590  [76864/150000]\n",
      "loss: 0.106274  [83264/150000]\n",
      "loss: 0.111732  [89664/150000]\n",
      "loss: 0.040235  [96064/150000]\n",
      "loss: 0.881845  [102464/150000]\n",
      "loss: 0.860300  [108864/150000]\n",
      "loss: 1.170710  [115264/150000]\n",
      "loss: 0.814708  [121664/150000]\n",
      "loss: 0.833823  [128064/150000]\n",
      "loss: 0.851990  [134464/150000]\n",
      "loss: 0.822254  [140864/150000]\n",
      "loss: 0.880583  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.805574 \n",
      "\n",
      "training epoch 65\n",
      "loss: 0.256956  [   64/150000]\n",
      "loss: 0.295496  [ 6464/150000]\n",
      "loss: 0.071004  [12864/150000]\n",
      "loss: 0.256764  [19264/150000]\n",
      "loss: 0.169098  [25664/150000]\n",
      "loss: 0.365908  [32064/150000]\n",
      "loss: 0.141324  [38464/150000]\n",
      "loss: 0.309998  [44864/150000]\n",
      "loss: 0.065073  [51264/150000]\n",
      "loss: 0.100665  [57664/150000]\n",
      "loss: 0.174280  [64064/150000]\n",
      "loss: 0.050354  [70464/150000]\n",
      "loss: 0.018663  [76864/150000]\n",
      "loss: 0.045823  [83264/150000]\n",
      "loss: 0.075779  [89664/150000]\n",
      "loss: 0.050326  [96064/150000]\n",
      "loss: 0.821134  [102464/150000]\n",
      "loss: 1.071156  [108864/150000]\n",
      "loss: 0.796231  [115264/150000]\n",
      "loss: 0.769750  [121664/150000]\n",
      "loss: 1.063108  [128064/150000]\n",
      "loss: 0.814303  [134464/150000]\n",
      "loss: 1.156262  [140864/150000]\n",
      "loss: 1.118580  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.823358 \n",
      "\n",
      "training epoch 66\n",
      "loss: 0.298005  [   64/150000]\n",
      "loss: 0.134154  [ 6464/150000]\n",
      "loss: 0.063962  [12864/150000]\n",
      "loss: 0.098715  [19264/150000]\n",
      "loss: 0.130072  [25664/150000]\n",
      "loss: 0.085279  [32064/150000]\n",
      "loss: 0.264483  [38464/150000]\n",
      "loss: 0.243767  [44864/150000]\n",
      "loss: 0.123698  [51264/150000]\n",
      "loss: 0.073214  [57664/150000]\n",
      "loss: 0.122066  [64064/150000]\n",
      "loss: 0.086023  [70464/150000]\n",
      "loss: 0.275114  [76864/150000]\n",
      "loss: 0.057234  [83264/150000]\n",
      "loss: 0.028623  [89664/150000]\n",
      "loss: 0.158087  [96064/150000]\n",
      "loss: 0.909032  [102464/150000]\n",
      "loss: 0.858154  [108864/150000]\n",
      "loss: 1.147011  [115264/150000]\n",
      "loss: 0.921657  [121664/150000]\n",
      "loss: 0.712430  [128064/150000]\n",
      "loss: 0.939454  [134464/150000]\n",
      "loss: 1.013186  [140864/150000]\n",
      "loss: 0.668068  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.788445 \n",
      "\n",
      "training epoch 67\n",
      "loss: 0.287428  [   64/150000]\n",
      "loss: 0.229198  [ 6464/150000]\n",
      "loss: 0.074251  [12864/150000]\n",
      "loss: 0.181008  [19264/150000]\n",
      "loss: 0.049090  [25664/150000]\n",
      "loss: 0.058518  [32064/150000]\n",
      "loss: 0.091769  [38464/150000]\n",
      "loss: 0.106575  [44864/150000]\n",
      "loss: 0.094504  [51264/150000]\n",
      "loss: 0.019412  [57664/150000]\n",
      "loss: 0.068655  [64064/150000]\n",
      "loss: 0.076826  [70464/150000]\n",
      "loss: 0.052098  [76864/150000]\n",
      "loss: 0.070517  [83264/150000]\n",
      "loss: 0.044740  [89664/150000]\n",
      "loss: 0.055962  [96064/150000]\n",
      "loss: 1.177118  [102464/150000]\n",
      "loss: 1.156284  [108864/150000]\n",
      "loss: 1.055264  [115264/150000]\n",
      "loss: 0.919249  [121664/150000]\n",
      "loss: 0.810480  [128064/150000]\n",
      "loss: 0.902048  [134464/150000]\n",
      "loss: 0.998038  [140864/150000]\n",
      "loss: 0.853910  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.829580 \n",
      "\n",
      "training epoch 68\n",
      "loss: 0.339791  [   64/150000]\n",
      "loss: 0.054539  [ 6464/150000]\n",
      "loss: 0.082649  [12864/150000]\n",
      "loss: 0.150748  [19264/150000]\n",
      "loss: 0.091048  [25664/150000]\n",
      "loss: 0.100177  [32064/150000]\n",
      "loss: 0.051327  [38464/150000]\n",
      "loss: 0.091635  [44864/150000]\n",
      "loss: 0.061279  [51264/150000]\n",
      "loss: 0.053954  [57664/150000]\n",
      "loss: 0.058664  [64064/150000]\n",
      "loss: 0.154509  [70464/150000]\n",
      "loss: 0.064849  [76864/150000]\n",
      "loss: 0.057313  [83264/150000]\n",
      "loss: 0.122174  [89664/150000]\n",
      "loss: 0.049113  [96064/150000]\n",
      "loss: 1.125467  [102464/150000]\n",
      "loss: 0.881617  [108864/150000]\n",
      "loss: 1.149760  [115264/150000]\n",
      "loss: 0.774983  [121664/150000]\n",
      "loss: 0.864726  [128064/150000]\n",
      "loss: 1.006740  [134464/150000]\n",
      "loss: 0.921766  [140864/150000]\n",
      "loss: 1.011730  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.803062 \n",
      "\n",
      "training epoch 69\n",
      "loss: 0.349319  [   64/150000]\n",
      "loss: 0.046542  [ 6464/150000]\n",
      "loss: 0.046413  [12864/150000]\n",
      "loss: 0.335855  [19264/150000]\n",
      "loss: 0.122246  [25664/150000]\n",
      "loss: 0.153481  [32064/150000]\n",
      "loss: 0.251108  [38464/150000]\n",
      "loss: 0.051577  [44864/150000]\n",
      "loss: 0.018360  [51264/150000]\n",
      "loss: 0.039161  [57664/150000]\n",
      "loss: 0.026252  [64064/150000]\n",
      "loss: 0.307105  [70464/150000]\n",
      "loss: 0.078296  [76864/150000]\n",
      "loss: 0.023603  [83264/150000]\n",
      "loss: 0.110993  [89664/150000]\n",
      "loss: 0.170633  [96064/150000]\n",
      "loss: 0.818686  [102464/150000]\n",
      "loss: 1.134420  [108864/150000]\n",
      "loss: 1.232928  [115264/150000]\n",
      "loss: 0.834390  [121664/150000]\n",
      "loss: 0.824623  [128064/150000]\n",
      "loss: 1.145173  [134464/150000]\n",
      "loss: 0.769554  [140864/150000]\n",
      "loss: 0.724083  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.808306 \n",
      "\n",
      "training epoch 70\n",
      "loss: 0.219626  [   64/150000]\n",
      "loss: 0.195768  [ 6464/150000]\n",
      "loss: 0.107144  [12864/150000]\n",
      "loss: 0.071037  [19264/150000]\n",
      "loss: 0.142843  [25664/150000]\n",
      "loss: 0.087379  [32064/150000]\n",
      "loss: 0.049090  [38464/150000]\n",
      "loss: 0.040322  [44864/150000]\n",
      "loss: 0.056454  [51264/150000]\n",
      "loss: 0.068005  [57664/150000]\n",
      "loss: 0.097257  [64064/150000]\n",
      "loss: 0.089096  [70464/150000]\n",
      "loss: 0.007753  [76864/150000]\n",
      "loss: 0.069992  [83264/150000]\n",
      "loss: 0.023828  [89664/150000]\n",
      "loss: 0.146129  [96064/150000]\n",
      "loss: 0.992801  [102464/150000]\n",
      "loss: 0.907693  [108864/150000]\n",
      "loss: 1.085575  [115264/150000]\n",
      "loss: 0.845645  [121664/150000]\n",
      "loss: 0.940041  [128064/150000]\n",
      "loss: 0.949146  [134464/150000]\n",
      "loss: 0.979005  [140864/150000]\n",
      "loss: 0.598893  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.764506 \n",
      "\n",
      "training epoch 71\n",
      "loss: 0.193451  [   64/150000]\n",
      "loss: 0.056873  [ 6464/150000]\n",
      "loss: 0.100138  [12864/150000]\n",
      "loss: 0.069594  [19264/150000]\n",
      "loss: 0.059175  [25664/150000]\n",
      "loss: 0.048391  [32064/150000]\n",
      "loss: 0.160417  [38464/150000]\n",
      "loss: 0.038535  [44864/150000]\n",
      "loss: 0.019844  [51264/150000]\n",
      "loss: 0.014960  [57664/150000]\n",
      "loss: 0.166126  [64064/150000]\n",
      "loss: 0.038728  [70464/150000]\n",
      "loss: 0.151933  [76864/150000]\n",
      "loss: 0.130982  [83264/150000]\n",
      "loss: 0.032052  [89664/150000]\n",
      "loss: 0.090959  [96064/150000]\n",
      "loss: 0.845422  [102464/150000]\n",
      "loss: 1.181454  [108864/150000]\n",
      "loss: 1.067661  [115264/150000]\n",
      "loss: 0.786457  [121664/150000]\n",
      "loss: 0.997056  [128064/150000]\n",
      "loss: 1.009228  [134464/150000]\n",
      "loss: 1.171239  [140864/150000]\n",
      "loss: 0.858679  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.817195 \n",
      "\n",
      "training epoch 72\n",
      "loss: 0.347067  [   64/150000]\n",
      "loss: 0.073636  [ 6464/150000]\n",
      "loss: 0.083792  [12864/150000]\n",
      "loss: 0.133299  [19264/150000]\n",
      "loss: 0.097353  [25664/150000]\n",
      "loss: 0.093131  [32064/150000]\n",
      "loss: 0.133684  [38464/150000]\n",
      "loss: 0.064806  [44864/150000]\n",
      "loss: 0.031746  [51264/150000]\n",
      "loss: 0.154119  [57664/150000]\n",
      "loss: 0.059549  [64064/150000]\n",
      "loss: 0.098212  [70464/150000]\n",
      "loss: 0.050499  [76864/150000]\n",
      "loss: 0.142957  [83264/150000]\n",
      "loss: 0.069964  [89664/150000]\n",
      "loss: 0.075689  [96064/150000]\n",
      "loss: 0.703059  [102464/150000]\n",
      "loss: 1.002271  [108864/150000]\n",
      "loss: 1.063254  [115264/150000]\n",
      "loss: 0.716603  [121664/150000]\n",
      "loss: 0.921870  [128064/150000]\n",
      "loss: 1.117021  [134464/150000]\n",
      "loss: 0.840634  [140864/150000]\n",
      "loss: 0.950621  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.811857 \n",
      "\n",
      "training epoch 73\n",
      "loss: 0.296116  [   64/150000]\n",
      "loss: 0.094752  [ 6464/150000]\n",
      "loss: 0.178491  [12864/150000]\n",
      "loss: 0.359236  [19264/150000]\n",
      "loss: 0.053970  [25664/150000]\n",
      "loss: 0.028436  [32064/150000]\n",
      "loss: 0.212246  [38464/150000]\n",
      "loss: 0.116460  [44864/150000]\n",
      "loss: 0.080352  [51264/150000]\n",
      "loss: 0.032056  [57664/150000]\n",
      "loss: 0.026396  [64064/150000]\n",
      "loss: 0.039049  [70464/150000]\n",
      "loss: 0.013730  [76864/150000]\n",
      "loss: 0.009199  [83264/150000]\n",
      "loss: 0.117440  [89664/150000]\n",
      "loss: 0.054331  [96064/150000]\n",
      "loss: 1.013101  [102464/150000]\n",
      "loss: 0.841085  [108864/150000]\n",
      "loss: 0.921655  [115264/150000]\n",
      "loss: 0.767206  [121664/150000]\n",
      "loss: 0.957945  [128064/150000]\n",
      "loss: 1.141195  [134464/150000]\n",
      "loss: 1.121496  [140864/150000]\n",
      "loss: 1.123607  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.860236 \n",
      "\n",
      "training epoch 74\n",
      "loss: 0.322012  [   64/150000]\n",
      "loss: 0.087521  [ 6464/150000]\n",
      "loss: 0.032261  [12864/150000]\n",
      "loss: 0.160107  [19264/150000]\n",
      "loss: 0.101178  [25664/150000]\n",
      "loss: 0.177436  [32064/150000]\n",
      "loss: 0.081207  [38464/150000]\n",
      "loss: 0.309376  [44864/150000]\n",
      "loss: 0.032142  [51264/150000]\n",
      "loss: 0.137145  [57664/150000]\n",
      "loss: 0.076146  [64064/150000]\n",
      "loss: 0.069069  [70464/150000]\n",
      "loss: 0.017303  [76864/150000]\n",
      "loss: 0.045303  [83264/150000]\n",
      "loss: 0.022940  [89664/150000]\n",
      "loss: 0.023859  [96064/150000]\n",
      "loss: 1.139637  [102464/150000]\n",
      "loss: 0.948148  [108864/150000]\n",
      "loss: 0.860407  [115264/150000]\n",
      "loss: 0.890185  [121664/150000]\n",
      "loss: 0.893969  [128064/150000]\n",
      "loss: 0.922141  [134464/150000]\n",
      "loss: 1.150164  [140864/150000]\n",
      "loss: 0.876220  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.845715 \n",
      "\n",
      "training epoch 75\n",
      "loss: 0.151522  [   64/150000]\n",
      "loss: 0.043044  [ 6464/150000]\n",
      "loss: 0.143357  [12864/150000]\n",
      "loss: 0.042175  [19264/150000]\n",
      "loss: 0.112129  [25664/150000]\n",
      "loss: 0.115204  [32064/150000]\n",
      "loss: 0.042282  [38464/150000]\n",
      "loss: 0.219225  [44864/150000]\n",
      "loss: 0.027764  [51264/150000]\n",
      "loss: 0.057946  [57664/150000]\n",
      "loss: 0.169386  [64064/150000]\n",
      "loss: 0.107442  [70464/150000]\n",
      "loss: 0.016386  [76864/150000]\n",
      "loss: 0.212877  [83264/150000]\n",
      "loss: 0.112997  [89664/150000]\n",
      "loss: 0.122837  [96064/150000]\n",
      "loss: 1.001426  [102464/150000]\n",
      "loss: 0.975077  [108864/150000]\n",
      "loss: 1.079166  [115264/150000]\n",
      "loss: 0.686183  [121664/150000]\n",
      "loss: 0.922596  [128064/150000]\n",
      "loss: 0.981879  [134464/150000]\n",
      "loss: 0.852669  [140864/150000]\n",
      "loss: 0.967472  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.777858 \n",
      "\n",
      "training epoch 76\n",
      "loss: 0.304939  [   64/150000]\n",
      "loss: 0.033321  [ 6464/150000]\n",
      "loss: 0.052074  [12864/150000]\n",
      "loss: 0.149496  [19264/150000]\n",
      "loss: 0.029634  [25664/150000]\n",
      "loss: 0.193027  [32064/150000]\n",
      "loss: 0.051581  [38464/150000]\n",
      "loss: 0.078396  [44864/150000]\n",
      "loss: 0.030156  [51264/150000]\n",
      "loss: 0.107795  [57664/150000]\n",
      "loss: 0.020870  [64064/150000]\n",
      "loss: 0.088994  [70464/150000]\n",
      "loss: 0.070842  [76864/150000]\n",
      "loss: 0.026439  [83264/150000]\n",
      "loss: 0.048266  [89664/150000]\n",
      "loss: 0.086885  [96064/150000]\n",
      "loss: 0.846753  [102464/150000]\n",
      "loss: 1.126920  [108864/150000]\n",
      "loss: 1.137170  [115264/150000]\n",
      "loss: 0.814467  [121664/150000]\n",
      "loss: 0.896376  [128064/150000]\n",
      "loss: 1.193763  [134464/150000]\n",
      "loss: 0.934906  [140864/150000]\n",
      "loss: 0.737868  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.794326 \n",
      "\n",
      "training epoch 77\n",
      "loss: 0.229509  [   64/150000]\n",
      "loss: 0.087219  [ 6464/150000]\n",
      "loss: 0.065897  [12864/150000]\n",
      "loss: 0.294834  [19264/150000]\n",
      "loss: 0.212081  [25664/150000]\n",
      "loss: 0.134729  [32064/150000]\n",
      "loss: 0.080193  [38464/150000]\n",
      "loss: 0.174721  [44864/150000]\n",
      "loss: 0.084048  [51264/150000]\n",
      "loss: 0.020344  [57664/150000]\n",
      "loss: 0.077751  [64064/150000]\n",
      "loss: 0.022829  [70464/150000]\n",
      "loss: 0.044263  [76864/150000]\n",
      "loss: 0.184130  [83264/150000]\n",
      "loss: 0.078527  [89664/150000]\n",
      "loss: 0.044464  [96064/150000]\n",
      "loss: 0.998197  [102464/150000]\n",
      "loss: 1.018523  [108864/150000]\n",
      "loss: 1.068954  [115264/150000]\n",
      "loss: 0.681544  [121664/150000]\n",
      "loss: 0.927341  [128064/150000]\n",
      "loss: 1.126043  [134464/150000]\n",
      "loss: 1.025937  [140864/150000]\n",
      "loss: 0.717425  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.833181 \n",
      "\n",
      "training epoch 78\n",
      "loss: 0.302122  [   64/150000]\n",
      "loss: 0.164149  [ 6464/150000]\n",
      "loss: 0.121807  [12864/150000]\n",
      "loss: 0.038873  [19264/150000]\n",
      "loss: 0.025075  [25664/150000]\n",
      "loss: 0.083623  [32064/150000]\n",
      "loss: 0.148686  [38464/150000]\n",
      "loss: 0.064234  [44864/150000]\n",
      "loss: 0.037887  [51264/150000]\n",
      "loss: 0.044251  [57664/150000]\n",
      "loss: 0.177510  [64064/150000]\n",
      "loss: 0.033464  [70464/150000]\n",
      "loss: 0.210613  [76864/150000]\n",
      "loss: 0.019013  [83264/150000]\n",
      "loss: 0.036097  [89664/150000]\n",
      "loss: 0.027735  [96064/150000]\n",
      "loss: 0.895007  [102464/150000]\n",
      "loss: 1.067969  [108864/150000]\n",
      "loss: 0.921886  [115264/150000]\n",
      "loss: 0.760180  [121664/150000]\n",
      "loss: 0.727097  [128064/150000]\n",
      "loss: 1.068025  [134464/150000]\n",
      "loss: 0.774099  [140864/150000]\n",
      "loss: 1.020906  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.797832 \n",
      "\n",
      "training epoch 79\n",
      "loss: 0.220155  [   64/150000]\n",
      "loss: 0.216406  [ 6464/150000]\n",
      "loss: 0.092554  [12864/150000]\n",
      "loss: 0.180641  [19264/150000]\n",
      "loss: 0.102330  [25664/150000]\n",
      "loss: 0.068132  [32064/150000]\n",
      "loss: 0.122944  [38464/150000]\n",
      "loss: 0.209202  [44864/150000]\n",
      "loss: 0.027730  [51264/150000]\n",
      "loss: 0.053908  [57664/150000]\n",
      "loss: 0.223931  [64064/150000]\n",
      "loss: 0.130203  [70464/150000]\n",
      "loss: 0.009046  [76864/150000]\n",
      "loss: 0.026474  [83264/150000]\n",
      "loss: 0.073900  [89664/150000]\n",
      "loss: 0.066448  [96064/150000]\n",
      "loss: 1.010224  [102464/150000]\n",
      "loss: 0.998762  [108864/150000]\n",
      "loss: 0.909915  [115264/150000]\n",
      "loss: 0.899881  [121664/150000]\n",
      "loss: 0.820193  [128064/150000]\n",
      "loss: 1.045923  [134464/150000]\n",
      "loss: 0.961592  [140864/150000]\n",
      "loss: 0.768090  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.802512 \n",
      "\n",
      "training epoch 80\n",
      "loss: 0.234404  [   64/150000]\n",
      "loss: 0.064590  [ 6464/150000]\n",
      "loss: 0.069469  [12864/150000]\n",
      "loss: 0.309236  [19264/150000]\n",
      "loss: 0.066120  [25664/150000]\n",
      "loss: 0.169665  [32064/150000]\n",
      "loss: 0.061793  [38464/150000]\n",
      "loss: 0.059760  [44864/150000]\n",
      "loss: 0.054007  [51264/150000]\n",
      "loss: 0.019175  [57664/150000]\n",
      "loss: 0.086712  [64064/150000]\n",
      "loss: 0.018301  [70464/150000]\n",
      "loss: 0.036320  [76864/150000]\n",
      "loss: 0.022526  [83264/150000]\n",
      "loss: 0.125948  [89664/150000]\n",
      "loss: 0.047362  [96064/150000]\n",
      "loss: 0.993837  [102464/150000]\n",
      "loss: 0.965130  [108864/150000]\n",
      "loss: 1.095870  [115264/150000]\n",
      "loss: 0.684287  [121664/150000]\n",
      "loss: 1.108076  [128064/150000]\n",
      "loss: 0.879504  [134464/150000]\n",
      "loss: 0.979068  [140864/150000]\n",
      "loss: 0.837234  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.797503 \n",
      "\n",
      "training epoch 81\n",
      "loss: 0.208205  [   64/150000]\n",
      "loss: 0.035234  [ 6464/150000]\n",
      "loss: 0.036509  [12864/150000]\n",
      "loss: 0.217346  [19264/150000]\n",
      "loss: 0.028419  [25664/150000]\n",
      "loss: 0.059967  [32064/150000]\n",
      "loss: 0.063227  [38464/150000]\n",
      "loss: 0.069361  [44864/150000]\n",
      "loss: 0.072827  [51264/150000]\n",
      "loss: 0.104479  [57664/150000]\n",
      "loss: 0.022677  [64064/150000]\n",
      "loss: 0.024801  [70464/150000]\n",
      "loss: 0.014947  [76864/150000]\n",
      "loss: 0.038852  [83264/150000]\n",
      "loss: 0.062126  [89664/150000]\n",
      "loss: 0.057169  [96064/150000]\n",
      "loss: 0.828073  [102464/150000]\n",
      "loss: 1.037669  [108864/150000]\n",
      "loss: 0.960352  [115264/150000]\n",
      "loss: 0.809133  [121664/150000]\n",
      "loss: 0.797120  [128064/150000]\n",
      "loss: 1.149201  [134464/150000]\n",
      "loss: 0.790853  [140864/150000]\n",
      "loss: 0.752115  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.822256 \n",
      "\n",
      "training epoch 82\n",
      "loss: 0.160285  [   64/150000]\n",
      "loss: 0.132201  [ 6464/150000]\n",
      "loss: 0.117729  [12864/150000]\n",
      "loss: 0.107543  [19264/150000]\n",
      "loss: 0.176960  [25664/150000]\n",
      "loss: 0.027298  [32064/150000]\n",
      "loss: 0.355608  [38464/150000]\n",
      "loss: 0.031355  [44864/150000]\n",
      "loss: 0.169814  [51264/150000]\n",
      "loss: 0.029285  [57664/150000]\n",
      "loss: 0.014729  [64064/150000]\n",
      "loss: 0.036083  [70464/150000]\n",
      "loss: 0.043148  [76864/150000]\n",
      "loss: 0.086730  [83264/150000]\n",
      "loss: 0.084222  [89664/150000]\n",
      "loss: 0.155320  [96064/150000]\n",
      "loss: 1.061842  [102464/150000]\n",
      "loss: 0.955824  [108864/150000]\n",
      "loss: 0.890473  [115264/150000]\n",
      "loss: 1.060278  [121664/150000]\n",
      "loss: 0.769326  [128064/150000]\n",
      "loss: 1.060165  [134464/150000]\n",
      "loss: 1.074567  [140864/150000]\n",
      "loss: 0.847477  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.818697 \n",
      "\n",
      "training epoch 83\n",
      "loss: 0.269193  [   64/150000]\n",
      "loss: 0.197292  [ 6464/150000]\n",
      "loss: 0.051237  [12864/150000]\n",
      "loss: 0.232914  [19264/150000]\n",
      "loss: 0.157428  [25664/150000]\n",
      "loss: 0.098596  [32064/150000]\n",
      "loss: 0.179524  [38464/150000]\n",
      "loss: 0.071941  [44864/150000]\n",
      "loss: 0.032902  [51264/150000]\n",
      "loss: 0.035638  [57664/150000]\n",
      "loss: 0.161038  [64064/150000]\n",
      "loss: 0.054595  [70464/150000]\n",
      "loss: 0.180642  [76864/150000]\n",
      "loss: 0.012764  [83264/150000]\n",
      "loss: 0.049718  [89664/150000]\n",
      "loss: 0.050539  [96064/150000]\n",
      "loss: 1.099568  [102464/150000]\n",
      "loss: 0.742732  [108864/150000]\n",
      "loss: 1.251167  [115264/150000]\n",
      "loss: 0.919064  [121664/150000]\n",
      "loss: 0.958073  [128064/150000]\n",
      "loss: 0.826316  [134464/150000]\n",
      "loss: 0.871996  [140864/150000]\n",
      "loss: 0.919176  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.818794 \n",
      "\n",
      "training epoch 84\n",
      "loss: 0.218242  [   64/150000]\n",
      "loss: 0.027723  [ 6464/150000]\n",
      "loss: 0.057319  [12864/150000]\n",
      "loss: 0.066542  [19264/150000]\n",
      "loss: 0.109712  [25664/150000]\n",
      "loss: 0.087232  [32064/150000]\n",
      "loss: 0.144848  [38464/150000]\n",
      "loss: 0.009727  [44864/150000]\n",
      "loss: 0.227349  [51264/150000]\n",
      "loss: 0.025739  [57664/150000]\n",
      "loss: 0.016914  [64064/150000]\n",
      "loss: 0.069554  [70464/150000]\n",
      "loss: 0.069411  [76864/150000]\n",
      "loss: 0.075758  [83264/150000]\n",
      "loss: 0.090233  [89664/150000]\n",
      "loss: 0.225537  [96064/150000]\n",
      "loss: 0.873139  [102464/150000]\n",
      "loss: 1.049203  [108864/150000]\n",
      "loss: 1.005600  [115264/150000]\n",
      "loss: 0.749018  [121664/150000]\n",
      "loss: 0.836279  [128064/150000]\n",
      "loss: 1.066115  [134464/150000]\n",
      "loss: 0.980604  [140864/150000]\n",
      "loss: 0.889142  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.801439 \n",
      "\n",
      "training epoch 85\n",
      "loss: 0.273463  [   64/150000]\n",
      "loss: 0.144259  [ 6464/150000]\n",
      "loss: 0.037891  [12864/150000]\n",
      "loss: 0.130473  [19264/150000]\n",
      "loss: 0.090218  [25664/150000]\n",
      "loss: 0.172293  [32064/150000]\n",
      "loss: 0.031395  [38464/150000]\n",
      "loss: 0.220436  [44864/150000]\n",
      "loss: 0.073943  [51264/150000]\n",
      "loss: 0.030572  [57664/150000]\n",
      "loss: 0.028096  [64064/150000]\n",
      "loss: 0.092615  [70464/150000]\n",
      "loss: 0.085706  [76864/150000]\n",
      "loss: 0.017343  [83264/150000]\n",
      "loss: 0.114497  [89664/150000]\n",
      "loss: 0.070777  [96064/150000]\n",
      "loss: 1.052251  [102464/150000]\n",
      "loss: 0.692975  [108864/150000]\n",
      "loss: 1.137668  [115264/150000]\n",
      "loss: 0.852370  [121664/150000]\n",
      "loss: 1.087126  [128064/150000]\n",
      "loss: 1.158000  [134464/150000]\n",
      "loss: 1.115205  [140864/150000]\n",
      "loss: 0.891397  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.803076 \n",
      "\n",
      "training epoch 86\n",
      "loss: 0.320850  [   64/150000]\n",
      "loss: 0.148627  [ 6464/150000]\n",
      "loss: 0.042822  [12864/150000]\n",
      "loss: 0.096274  [19264/150000]\n",
      "loss: 0.236426  [25664/150000]\n",
      "loss: 0.141330  [32064/150000]\n",
      "loss: 0.153856  [38464/150000]\n",
      "loss: 0.050435  [44864/150000]\n",
      "loss: 0.068030  [51264/150000]\n",
      "loss: 0.037566  [57664/150000]\n",
      "loss: 0.386461  [64064/150000]\n",
      "loss: 0.024099  [70464/150000]\n",
      "loss: 0.114139  [76864/150000]\n",
      "loss: 0.046436  [83264/150000]\n",
      "loss: 0.096368  [89664/150000]\n",
      "loss: 0.078243  [96064/150000]\n",
      "loss: 0.987513  [102464/150000]\n",
      "loss: 0.861490  [108864/150000]\n",
      "loss: 0.902818  [115264/150000]\n",
      "loss: 0.787702  [121664/150000]\n",
      "loss: 0.689655  [128064/150000]\n",
      "loss: 0.939357  [134464/150000]\n",
      "loss: 1.003514  [140864/150000]\n",
      "loss: 0.926482  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.835794 \n",
      "\n",
      "training epoch 87\n",
      "loss: 0.279548  [   64/150000]\n",
      "loss: 0.043159  [ 6464/150000]\n",
      "loss: 0.057684  [12864/150000]\n",
      "loss: 0.097949  [19264/150000]\n",
      "loss: 0.071237  [25664/150000]\n",
      "loss: 0.117253  [32064/150000]\n",
      "loss: 0.151703  [38464/150000]\n",
      "loss: 0.051766  [44864/150000]\n",
      "loss: 0.086976  [51264/150000]\n",
      "loss: 0.047280  [57664/150000]\n",
      "loss: 0.040635  [64064/150000]\n",
      "loss: 0.122125  [70464/150000]\n",
      "loss: 0.036144  [76864/150000]\n",
      "loss: 0.309295  [83264/150000]\n",
      "loss: 0.070170  [89664/150000]\n",
      "loss: 0.097026  [96064/150000]\n",
      "loss: 1.023562  [102464/150000]\n",
      "loss: 0.790847  [108864/150000]\n",
      "loss: 1.026733  [115264/150000]\n",
      "loss: 0.935030  [121664/150000]\n",
      "loss: 0.938187  [128064/150000]\n",
      "loss: 1.050200  [134464/150000]\n",
      "loss: 0.819813  [140864/150000]\n",
      "loss: 0.702738  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.800078 \n",
      "\n",
      "training epoch 88\n",
      "loss: 0.322636  [   64/150000]\n",
      "loss: 0.110710  [ 6464/150000]\n",
      "loss: 0.191597  [12864/150000]\n",
      "loss: 0.062628  [19264/150000]\n",
      "loss: 0.086683  [25664/150000]\n",
      "loss: 0.265379  [32064/150000]\n",
      "loss: 0.040999  [38464/150000]\n",
      "loss: 0.329454  [44864/150000]\n",
      "loss: 0.110732  [51264/150000]\n",
      "loss: 0.009747  [57664/150000]\n",
      "loss: 0.057414  [64064/150000]\n",
      "loss: 0.048904  [70464/150000]\n",
      "loss: 0.079740  [76864/150000]\n",
      "loss: 0.009277  [83264/150000]\n",
      "loss: 0.057926  [89664/150000]\n",
      "loss: 0.073647  [96064/150000]\n",
      "loss: 0.915346  [102464/150000]\n",
      "loss: 1.054040  [108864/150000]\n",
      "loss: 0.956459  [115264/150000]\n",
      "loss: 1.076058  [121664/150000]\n",
      "loss: 0.923167  [128064/150000]\n",
      "loss: 0.705474  [134464/150000]\n",
      "loss: 0.776945  [140864/150000]\n",
      "loss: 0.670314  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.814350 \n",
      "\n",
      "training epoch 89\n",
      "loss: 0.145189  [   64/150000]\n",
      "loss: 0.124774  [ 6464/150000]\n",
      "loss: 0.096982  [12864/150000]\n",
      "loss: 0.103396  [19264/150000]\n",
      "loss: 0.055606  [25664/150000]\n",
      "loss: 0.096499  [32064/150000]\n",
      "loss: 0.101850  [38464/150000]\n",
      "loss: 0.217158  [44864/150000]\n",
      "loss: 0.019676  [51264/150000]\n",
      "loss: 0.041758  [57664/150000]\n",
      "loss: 0.024988  [64064/150000]\n",
      "loss: 0.040247  [70464/150000]\n",
      "loss: 0.028215  [76864/150000]\n",
      "loss: 0.079045  [83264/150000]\n",
      "loss: 0.022049  [89664/150000]\n",
      "loss: 0.142575  [96064/150000]\n",
      "loss: 1.100698  [102464/150000]\n",
      "loss: 1.050916  [108864/150000]\n",
      "loss: 0.805308  [115264/150000]\n",
      "loss: 0.772156  [121664/150000]\n",
      "loss: 0.776838  [128064/150000]\n",
      "loss: 1.085520  [134464/150000]\n",
      "loss: 1.138134  [140864/150000]\n",
      "loss: 0.711011  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.814354 \n",
      "\n",
      "training epoch 90\n",
      "loss: 0.280628  [   64/150000]\n",
      "loss: 0.071178  [ 6464/150000]\n",
      "loss: 0.027428  [12864/150000]\n",
      "loss: 0.117118  [19264/150000]\n",
      "loss: 0.067900  [25664/150000]\n",
      "loss: 0.045005  [32064/150000]\n",
      "loss: 0.148578  [38464/150000]\n",
      "loss: 0.077874  [44864/150000]\n",
      "loss: 0.079038  [51264/150000]\n",
      "loss: 0.099291  [57664/150000]\n",
      "loss: 0.093218  [64064/150000]\n",
      "loss: 0.067795  [70464/150000]\n",
      "loss: 0.165288  [76864/150000]\n",
      "loss: 0.029125  [83264/150000]\n",
      "loss: 0.033075  [89664/150000]\n",
      "loss: 0.021429  [96064/150000]\n",
      "loss: 0.904844  [102464/150000]\n",
      "loss: 0.791521  [108864/150000]\n",
      "loss: 0.772139  [115264/150000]\n",
      "loss: 0.783927  [121664/150000]\n",
      "loss: 0.879488  [128064/150000]\n",
      "loss: 1.020065  [134464/150000]\n",
      "loss: 1.071823  [140864/150000]\n",
      "loss: 0.778956  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.794371 \n",
      "\n",
      "training epoch 91\n",
      "loss: 0.223539  [   64/150000]\n",
      "loss: 0.176473  [ 6464/150000]\n",
      "loss: 0.147123  [12864/150000]\n",
      "loss: 0.095889  [19264/150000]\n",
      "loss: 0.056183  [25664/150000]\n",
      "loss: 0.245441  [32064/150000]\n",
      "loss: 0.165199  [38464/150000]\n",
      "loss: 0.278789  [44864/150000]\n",
      "loss: 0.026041  [51264/150000]\n",
      "loss: 0.018518  [57664/150000]\n",
      "loss: 0.079243  [64064/150000]\n",
      "loss: 0.023543  [70464/150000]\n",
      "loss: 0.103266  [76864/150000]\n",
      "loss: 0.039273  [83264/150000]\n",
      "loss: 0.040566  [89664/150000]\n",
      "loss: 0.013065  [96064/150000]\n",
      "loss: 0.971627  [102464/150000]\n",
      "loss: 0.841506  [108864/150000]\n",
      "loss: 0.893126  [115264/150000]\n",
      "loss: 0.736388  [121664/150000]\n",
      "loss: 0.889332  [128064/150000]\n",
      "loss: 0.849611  [134464/150000]\n",
      "loss: 1.305145  [140864/150000]\n",
      "loss: 0.901390  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.812008 \n",
      "\n",
      "training epoch 92\n",
      "loss: 0.126408  [   64/150000]\n",
      "loss: 0.093670  [ 6464/150000]\n",
      "loss: 0.035102  [12864/150000]\n",
      "loss: 0.155100  [19264/150000]\n",
      "loss: 0.142742  [25664/150000]\n",
      "loss: 0.175138  [32064/150000]\n",
      "loss: 0.057069  [38464/150000]\n",
      "loss: 0.132767  [44864/150000]\n",
      "loss: 0.577730  [51264/150000]\n",
      "loss: 0.304682  [57664/150000]\n",
      "loss: 0.109038  [64064/150000]\n",
      "loss: 0.129024  [70464/150000]\n",
      "loss: 0.055916  [76864/150000]\n",
      "loss: 0.010831  [83264/150000]\n",
      "loss: 0.030006  [89664/150000]\n",
      "loss: 0.200361  [96064/150000]\n",
      "loss: 0.903877  [102464/150000]\n",
      "loss: 0.877065  [108864/150000]\n",
      "loss: 0.906864  [115264/150000]\n",
      "loss: 0.777155  [121664/150000]\n",
      "loss: 0.872469  [128064/150000]\n",
      "loss: 0.995107  [134464/150000]\n",
      "loss: 0.966455  [140864/150000]\n",
      "loss: 1.031042  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.798104 \n",
      "\n",
      "training epoch 93\n",
      "loss: 0.275653  [   64/150000]\n",
      "loss: 0.047396  [ 6464/150000]\n",
      "loss: 0.065095  [12864/150000]\n",
      "loss: 0.190049  [19264/150000]\n",
      "loss: 0.077331  [25664/150000]\n",
      "loss: 0.029041  [32064/150000]\n",
      "loss: 0.101081  [38464/150000]\n",
      "loss: 0.201893  [44864/150000]\n",
      "loss: 0.118506  [51264/150000]\n",
      "loss: 0.048564  [57664/150000]\n",
      "loss: 0.086738  [64064/150000]\n",
      "loss: 0.048069  [70464/150000]\n",
      "loss: 0.016920  [76864/150000]\n",
      "loss: 0.026604  [83264/150000]\n",
      "loss: 0.014322  [89664/150000]\n",
      "loss: 0.058122  [96064/150000]\n",
      "loss: 0.948878  [102464/150000]\n",
      "loss: 1.089421  [108864/150000]\n",
      "loss: 0.985481  [115264/150000]\n",
      "loss: 0.800938  [121664/150000]\n",
      "loss: 0.803879  [128064/150000]\n",
      "loss: 0.943222  [134464/150000]\n",
      "loss: 0.989484  [140864/150000]\n",
      "loss: 0.924593  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.802646 \n",
      "\n",
      "training epoch 94\n",
      "loss: 0.216150  [   64/150000]\n",
      "loss: 0.146853  [ 6464/150000]\n",
      "loss: 0.030227  [12864/150000]\n",
      "loss: 0.163380  [19264/150000]\n",
      "loss: 0.093142  [25664/150000]\n",
      "loss: 0.155321  [32064/150000]\n",
      "loss: 0.044044  [38464/150000]\n",
      "loss: 0.149797  [44864/150000]\n",
      "loss: 0.011169  [51264/150000]\n",
      "loss: 0.021112  [57664/150000]\n",
      "loss: 0.025745  [64064/150000]\n",
      "loss: 0.099739  [70464/150000]\n",
      "loss: 0.035396  [76864/150000]\n",
      "loss: 0.009053  [83264/150000]\n",
      "loss: 0.075622  [89664/150000]\n",
      "loss: 0.078472  [96064/150000]\n",
      "loss: 0.876925  [102464/150000]\n",
      "loss: 0.926933  [108864/150000]\n",
      "loss: 0.758095  [115264/150000]\n",
      "loss: 0.905430  [121664/150000]\n",
      "loss: 1.086007  [128064/150000]\n",
      "loss: 1.123547  [134464/150000]\n",
      "loss: 1.049625  [140864/150000]\n",
      "loss: 0.815690  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.794381 \n",
      "\n",
      "training epoch 95\n",
      "loss: 0.209590  [   64/150000]\n",
      "loss: 0.110115  [ 6464/150000]\n",
      "loss: 0.051343  [12864/150000]\n",
      "loss: 0.065078  [19264/150000]\n",
      "loss: 0.049099  [25664/150000]\n",
      "loss: 0.032196  [32064/150000]\n",
      "loss: 0.305342  [38464/150000]\n",
      "loss: 0.033842  [44864/150000]\n",
      "loss: 0.043552  [51264/150000]\n",
      "loss: 0.002756  [57664/150000]\n",
      "loss: 0.027597  [64064/150000]\n",
      "loss: 0.032685  [70464/150000]\n",
      "loss: 0.222032  [76864/150000]\n",
      "loss: 0.060700  [83264/150000]\n",
      "loss: 0.065758  [89664/150000]\n",
      "loss: 0.042469  [96064/150000]\n",
      "loss: 0.820131  [102464/150000]\n",
      "loss: 0.827243  [108864/150000]\n",
      "loss: 1.022026  [115264/150000]\n",
      "loss: 0.712678  [121664/150000]\n",
      "loss: 0.746244  [128064/150000]\n",
      "loss: 0.808209  [134464/150000]\n",
      "loss: 1.100036  [140864/150000]\n",
      "loss: 0.856252  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.879222 \n",
      "\n",
      "training epoch 96\n",
      "loss: 0.326284  [   64/150000]\n",
      "loss: 0.115436  [ 6464/150000]\n",
      "loss: 0.130419  [12864/150000]\n",
      "loss: 0.193814  [19264/150000]\n",
      "loss: 0.136134  [25664/150000]\n",
      "loss: 0.042422  [32064/150000]\n",
      "loss: 0.062706  [38464/150000]\n",
      "loss: 0.027643  [44864/150000]\n",
      "loss: 0.168707  [51264/150000]\n",
      "loss: 0.026783  [57664/150000]\n",
      "loss: 0.033907  [64064/150000]\n",
      "loss: 0.026425  [70464/150000]\n",
      "loss: 0.059553  [76864/150000]\n",
      "loss: 0.086879  [83264/150000]\n",
      "loss: 0.185120  [89664/150000]\n",
      "loss: 0.020650  [96064/150000]\n",
      "loss: 1.116370  [102464/150000]\n",
      "loss: 0.944851  [108864/150000]\n",
      "loss: 0.860130  [115264/150000]\n",
      "loss: 0.659151  [121664/150000]\n",
      "loss: 0.907997  [128064/150000]\n",
      "loss: 1.078210  [134464/150000]\n",
      "loss: 0.836834  [140864/150000]\n",
      "loss: 0.959260  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.784243 \n",
      "\n",
      "training epoch 97\n",
      "loss: 0.243349  [   64/150000]\n",
      "loss: 0.053488  [ 6464/150000]\n",
      "loss: 0.079337  [12864/150000]\n",
      "loss: 0.093162  [19264/150000]\n",
      "loss: 0.037202  [25664/150000]\n",
      "loss: 0.151339  [32064/150000]\n",
      "loss: 0.114369  [38464/150000]\n",
      "loss: 0.022272  [44864/150000]\n",
      "loss: 0.015949  [51264/150000]\n",
      "loss: 0.070293  [57664/150000]\n",
      "loss: 0.024460  [64064/150000]\n",
      "loss: 0.028786  [70464/150000]\n",
      "loss: 0.017455  [76864/150000]\n",
      "loss: 0.034866  [83264/150000]\n",
      "loss: 0.113901  [89664/150000]\n",
      "loss: 0.086180  [96064/150000]\n",
      "loss: 0.893551  [102464/150000]\n",
      "loss: 1.097803  [108864/150000]\n",
      "loss: 1.144331  [115264/150000]\n",
      "loss: 0.662241  [121664/150000]\n",
      "loss: 0.816838  [128064/150000]\n",
      "loss: 1.131218  [134464/150000]\n",
      "loss: 0.667527  [140864/150000]\n",
      "loss: 0.781043  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.868014 \n",
      "\n",
      "training epoch 98\n",
      "loss: 0.175612  [   64/150000]\n",
      "loss: 0.038602  [ 6464/150000]\n",
      "loss: 0.648464  [12864/150000]\n",
      "loss: 0.136326  [19264/150000]\n",
      "loss: 0.129827  [25664/150000]\n",
      "loss: 0.136995  [32064/150000]\n",
      "loss: 0.155628  [38464/150000]\n",
      "loss: 0.068390  [44864/150000]\n",
      "loss: 0.094221  [51264/150000]\n",
      "loss: 0.089978  [57664/150000]\n",
      "loss: 0.047338  [64064/150000]\n",
      "loss: 0.143275  [70464/150000]\n",
      "loss: 0.039841  [76864/150000]\n",
      "loss: 0.007897  [83264/150000]\n",
      "loss: 0.037529  [89664/150000]\n",
      "loss: 0.095189  [96064/150000]\n",
      "loss: 0.917808  [102464/150000]\n",
      "loss: 0.932678  [108864/150000]\n",
      "loss: 1.001674  [115264/150000]\n",
      "loss: 0.924426  [121664/150000]\n",
      "loss: 0.929863  [128064/150000]\n",
      "loss: 0.888366  [134464/150000]\n",
      "loss: 1.046613  [140864/150000]\n",
      "loss: 0.963947  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.825181 \n",
      "\n",
      "training epoch 99\n",
      "loss: 0.287788  [   64/150000]\n",
      "loss: 0.115048  [ 6464/150000]\n",
      "loss: 0.058157  [12864/150000]\n",
      "loss: 0.078158  [19264/150000]\n",
      "loss: 0.067186  [25664/150000]\n",
      "loss: 0.177492  [32064/150000]\n",
      "loss: 0.046250  [38464/150000]\n",
      "loss: 0.030792  [44864/150000]\n",
      "loss: 0.116447  [51264/150000]\n",
      "loss: 0.037441  [57664/150000]\n",
      "loss: 0.028063  [64064/150000]\n",
      "loss: 0.034149  [70464/150000]\n",
      "loss: 0.187318  [76864/150000]\n",
      "loss: 0.059001  [83264/150000]\n",
      "loss: 0.065897  [89664/150000]\n",
      "loss: 0.013081  [96064/150000]\n",
      "loss: 1.162970  [102464/150000]\n",
      "loss: 1.048469  [108864/150000]\n",
      "loss: 0.934613  [115264/150000]\n",
      "loss: 0.729765  [121664/150000]\n",
      "loss: 0.912158  [128064/150000]\n",
      "loss: 0.867877  [134464/150000]\n",
      "loss: 0.935082  [140864/150000]\n",
      "loss: 0.757178  [147264/150000]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.806967 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    print(f\"training epoch {e}\")\n",
    "    train_loop(train_dataloader, net, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, net, loss_fn)\n",
    "    if e % 10 == 0 or e == 99:\n",
    "        torch.save(net.state_dict(), '01.cifar-10.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
